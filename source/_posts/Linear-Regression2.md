---
title: 线性回归(2)——代价函数
date: 2017-11-21 15:27:50
tags: 机器学习-原理讲解
categories: 学习心得
mathjax: true
---

### 代价函数(cost function)

上一篇文章我们说到，线性回归需要找到一条直线来拟合所有样本点，那么我们如何来判断他是否拟合了呢？我们需要将直线和样本点的偏差量化，用一个函数来表达，之后求其最小值，就可以得到我们想要的参数了。而这个函数就叫做代价函数(cost function)。

为了方便后面的分析，下面我需要使用斯坦福机器学习公开课中的例子来进行解释：

| Living area (feet2) | bedrooms | Price (1000$s) |
| :-----------------: | :------: | :------------: |
|        2104         |    3     |      400       |
|        1600         |    3     |      330       |
|        2400         |    3     |      369       |
|        1416         |    2     |      232       |
|        3000         |    4     |      540       |
|         ……          |    ……    |       ……       |
|         ……          |    ……    |       ……       |

上图是一组房屋的数据，包含了房屋大小，卧室数量，以及售价。问题是，想要根据房屋大小以及卧室的数量来预测其售价，那么首先我们可以用一个线性方程来定义售价：

$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2
$$

h是hypotheses的简写。如果我们设$x_0=1$，那么上式可以简写为：

$$
h(x)=\sum_{i=0}^{n}\theta_ix_i=\theta^Tx
$$

其中$n=2$，表示的是特征数量，如果加入了其他和房价相关的特征，比如说离学校的距离，离地铁的距离等其他特征，n的数值会变大，式子形式还是一样。$\theta^T$和$x$为矩阵。而整个线性回归问题，就是为了确定一组参数$\theta^T$使得预测值和真实值的差异最小。

我们使用差的平方和来计算预测值和真实值之间的差异，所有差异之和就可以用下式表示：

$$
J(\theta)=\sum\_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2
$$

其中$h_\theta(x^{(i)})$代表第$i$个预测值，$y^{(i)}$代表第$i$个真实值。差异值最小，也就是$J(\theta)$最小。

这里大家可能会有疑问，为什么要用欧氏距离来计算差异？明明表示的是价钱差，直接相减取绝对值，不用平方不是可以更直观的表示吗？这个可以通过概率学来解释。

#### $J(\theta)$来历的解释

$h\_\theta(x)=\theta\_0+\theta\_1x\_1+\theta\_2x\_2$中的 $\theta\_0$ 是误差，误差表示与特征无关的常量影响，或者是随机噪音。 $h\_\theta(x)$ 可以写成下面的形式：

$$
y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}
$$

假设所有的$\epsilon^{(i)}$是独立同分布的，并且满服从均值为0，方差为$\sigma^2$的高斯分布。我们可以写出$\epsilon^{(i)}$的概率密度函数：
$$
p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}).
$$
我知道这个式子看起来一点也不简单，但是他实际上只是一个高斯分布的概率密度函数，如果你懂概率论那你一定不陌生，如果你不懂，没关系，暂时不去纠结为什么是这样，把他当作一个公式记住，并知道他描述的是概率就好。

因为$\epsilon^{(i)}=y^{(i)}-\theta^Tx^{(i)}$，把他带入，我们可以写出下面的表达式。
$$
p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}).
$$
上式也是一个概率密度函数，描述的是对于给定的$\theta$和$x^{(i)}$，$y^{(i)}$出现的概率。

##### 极大似然估计

这个东西有什么用？我们回到问题的出发点，想要找到一组参数，使得方程可以拟合所有的样本点。那么对于给定的参数来说，最好的那一组参数应该是会让目前现存的所有样本出现概率最大的那一组，这就是极大似然估计的中心想法。我们将所有样本在该参数下的出现的概率连乘，就可以得到likelihood函数，写为下式：

$$
L(\theta)=\prod\_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}).
$$

目的是求出该函数的最大值，但是连乘问题不好解决，我们通过对数函数将其转化为连加来解决：

$$
\begin{align}
l(\theta)&=\log L(\theta) \\\
&= \log \prod\_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\\
&=\sum\_{i=1}^{m}\log \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\\
&= m\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum\_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
\end{align}
$$

这个式子看起来很复杂，其实只用了对数的运算法则而已，第二步用的是$\log mn=\log m + \log n$，第三步用的是$\log m^n=n\log m$。相信大家稍微看一下就能明白是怎么回事。

因为该式第一项是个常数，所以只要使得第二项最小，就可以使得整体最大。而第二项……那不就和$J(\theta)$一样吗？这也就解释了为什么代价函数是真实值与预测值的差的平方和。

### 总结

本篇文章介绍了线性回归代价函数的由来，和最大似然估计法。
由于本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。

### Reference

《线性回归：机器学习史上最大命名错案》——机器学习入门微信公众号

斯坦福机器学习公开课讲义note1