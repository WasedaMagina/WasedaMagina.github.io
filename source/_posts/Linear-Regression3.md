---
title: 线性回归(3)——求最小值的两种方法
date: 2017-11-21 16:43:39
tags: 机器学习-原理讲解
categories: 学习心得
mathjax: true
---
### 求$J(\theta)$最小值

根据我们的假设，我们希望$J(\theta)$越小越好。求解$J(\theta)$的最小值，有两种方法，一种为梯度下降法，一种为最小二乘法。别看这两个名字都很不明觉厉，其实核心思想高中我们就学过——求导。

#### 梯度下降法
核心思想：因为x处的导数值为原函数在点(x,f(x))处切线的斜率，通过求导可以得到函数下降最快的方向，之后不断迭代，即可不断逼近函数的最小值。

对于上面这个例子来说，因为有多个$\theta$，我们需要对多个$\theta$求偏导，并按照下面的规则更新$\theta$值：
$$
\theta_j:=\theta_j-\alpha \frac {\partial}{\partial\theta_j}J(\theta)
$$
$\alpha$是学习率也叫步长，如果取值太大，容易跳过最小值，在最小值附近振荡。如果取值太小，会使得下降次数增多，导致收敛过慢。所以说步长这个别称还是挺有意思的，步子太小走得又慢，步子太大容易扯着蛋。

求偏导：
$$
\begin{align}
\frac {\partial}{\partial\theta\_j}J(\theta) &=\frac {\partial}{\partial\theta\_j}\sum\_{i=1}^m(h\_\theta(x^{(i)})-y^{(i)})^2 \\\
&= \sum\_{i=1}^m2(h\_\theta(x^{(i)})-y^{(i)})\frac {\partial}{\partial\theta\_j}(h\_\theta(x^{(i)})-y^{(i)}) \\\
&= \sum\_{i=1}^m(h\_\theta(x^{(i)})-y^{(i)}) x\_j
\end{align}
$$
有了偏导值，我们可以简单的写出该问题下梯度下降法的步骤：

不断重复下面这个式子，用来更新$\theta$值，直到收敛。
$$
\theta\_j=\theta\_j-\alpha \sum\_{i=1}^m(h\_\theta(x^{(i)})-y^{(i)}) x\_j
$$

#### 最小二乘法

核心思想：导数为0的地方为极值点，最小值就在那里。
这里你可能会有疑问，导数为零的地方只是极值点，有可能是极大值，极小值，不一定是最小值。
首先$J(\theta)$的形式决定了他只有最小值，没有最大值。其次$J(\theta)$是凸函数，所以他的局部极小值就是全局最小值
。至于$J(\theta)$为什么是凸函数。。。我不知道，反正他就是。Andrew Ng说的。
因为我们计算的$\theta$,$x$,$y$实际上都是矩阵，所以需要用到线性代数中求梯度的知识来计算，这部分我不打算细说，具体的推导步骤如下。

![微信图片_20171118182715](http://ozaeyj71y.bkt.clouddn.com/image/LR/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20171118182715.png)

然后令导数得零，即可获得$\theta$的值。

### 总结
线性回归因为内容比较多，分了三篇文章来叙述，现在已经基本说明白了，我们来总结一下：
1. 线性回归由高尔顿提出，其思想是几乎所有的科学观察都着了魔似的向平均值回归。
2. 线性回归的根本任务是确定一组参数，使得预测值和真实值之间的差异最小。
3. 可以通过极大似然估计来论证为什么要使用差的平方和来描述预测值和真实值之间的差异。
4. 极大似然估计认为对于给定的参数来说，最好的那一组应该是会让目前现存的所有样本出现概率最大的那组。
5. 我们可以使用梯度下降和最小二乘法来求解损失函数的最小值。
6. 梯度下降法的思想是过求导可以得到函数下降最快的方向，之后不断迭代，即可不断逼近函数的最小值。
7. 最小二乘法的思想是导数为0的地方为极值点，最小值就在那里。

最后，本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。

### Reference

《线性回归：机器学习史上最大命名错案》——机器学习入门微信公众号

斯坦福机器学习公开课讲义note1