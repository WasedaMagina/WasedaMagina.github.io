---
title: 决策树-基本原理
date: 2017-11-11 23:33:28
tags: 机器学习-原理讲解
categories: 学习心得
mathjax: true
---
> 一个事物内部会存在随机性，也就是不确定性，而从外部消除这个不确定性唯一的办法是引入信息。——吴军《数学之美》

### 什么是决策树

生活中人们其实常常在潜意识中用到该模型，只不过大家没意识到而已。

比如，新学期伊始，AB两人在宿舍里开始了闲谈：

A：咱们这学期选什么课啊，听学长说吉江老师的课挺好的，要不咱们一起上？

B：好啊，吉江课好过嘛

A：好过的，据说给分还挺高的。

B：那有考试吗？

A：没有考试，只用交5个实验报告就好。

B：作业呢？

A：不多，只用交实验报告，而且还是按组交的，组内要是有大神，嘿嘿嘿。

B：那是不是平时分特别多啊，压力会不会很大。

A：不会不会，吉江一天到晚往国内跑拉项目，就上六次课，压力约等于零。

B：那好啊，咱一起上吧。

这里B同学的决策逻辑就是一个典型的决策树，我们可以用下图表示。
![tree](http://ozaeyj71y.bkt.clouddn.com/image/decision_tree/tree.png)
可以看到，决策树其实就是一个用来做决策的树形结构。

正式的来定义一下决策树其实就是：

1. 是一个树形结构（二叉树或者多叉树）
2. 非叶结点（长方形）代表一个特征
3. 分支（箭头）代表输出
4. 叶节点（圆形）代表决策结果

### 决策树的优点

1. 非常简单，简单到你怀疑这到底是不是机器学习算法。
2. 非常通俗易懂，看到树就明白是怎么回事了。

### 如何造树

我们通过分析上图，能够提出两个问题：为什么B同学认为容不容易过，有没有考试，作业多不多，这三个要素是选择一门课的关键？以及为什么B同学为什么按照考试，作业，压力的顺序来进行提问。

正式点说就是：选取什么特征作为结点，以及用什么样的顺序来进行判断。这两点会成为构造决策树的关键。

### 决策树的构造

首先，我想明确一点：选取什么特征和给特征排序其实是一个问题。这个问题就是，什么特征，最为关键重要，最能左右我的决策。排序越靠后的特征，代表越不关键，越不能左右我的决定。既然如此，我就也没有必要将其加在决策树中了。

所以说，归根结底，我们只用解决一个问题：哪几个特征最能左右我的决策？这看上去是一个公说公有理婆说婆有理的问题，但是，恰好，我们有一个用来量化表示左右决策程度的量——信息熵增益。

通过简单的比较信息熵增益的大小，就可以判断出到底哪个特征更加重要。不过在介绍它之前，我需要先说明信息熵以及条件熵到底是什么。

#### 信息熵

熵这个概念并不是我们第一次接触了，早在高中时代我们就接触过热力学的熵，如果我没有记错的话，熵是一个用来描述物质/系统混乱程度的物理量，熵越高，越混乱。类似的，信息熵是一个描述随机变量的不确定性的物理量，信息熵越大，不确定性越高，信息量越少。不确定性和混乱度，这两组概念是多么像啊，难怪命名为信息熵。

设X是一个有限取值的离散随机变量，其概率分布为：
$$
P(X=x_i)=P_i,i=1,2,3...n
$$

则随机变量X的信息熵定义如下：

$$
H(X)=-\sum_{i=1}^{n}p_i \times log(p_i)
$$

这里我不想解释信息熵为什么定义成这个样子，这会牵扯很多和机器学习不相关的领域。我只想说明一下为什么这个式子可以用来表述不确定性，以及对于决策树来说不确定性为什么这么重要。

对于一个二元分类来说，即n=2的情况下，信息熵的定义式可以写成如下形式：

$$
H(X)=-p \times log(p)-(1-p) \times log(1-p)
$$

我们可以将H(X)理解为一个关于概率p的函数，其函数图像如下（摘自百度百科）：

![H(x)](http://ozaeyj71y.bkt.clouddn.com/image/decision_tree/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%9B%BE%E5%83%8F.jpg)

上文说过，信息熵H代表的是不确定性，H越大不确定性越大，信息量越小。若我们将上图的P理解为明天是晴天的概率，那么1-P代表的就是是明天不是晴天的概率。P=0就意味着明天百分百不是晴天，不确定性为0，所以P=0时H=0。同样的P=1时，意味着明天百分百是晴天，不确定性也为0，所以P=1时H=0。在P=0.5时，意味着我们对于明天是不是晴天毫无概念，不知道明天会是什么天，也不能像P=0.6时，得出明天很有可能是晴天的结论，我们仅仅知道我们一无所知。此时不确定性最大，信息量最小，所以H也在P=0.5时取到了最大值。

由此可见，信息熵的变化就代表了不确定性的变化。而决策树的构造过程其实就一个不断引入信息来降低不确定性的过程，每引入一个特征都会使得信息熵产生一定程度的减小，而熵减小的程度就代表了特征的重要性，这个熵减小的程度就是信息熵增益。

#### 条件熵

为了计算信息熵增益，首先要计算条件熵：

条件熵H(Y|X)表示在已知随机变量X的条件下，随机变量Y的不确定性。
$$
H(X|Y)=-\sum_{x\in X,y\in Y}P(x|y) \times log(x|y)
$$
换种说法就是，引入特征X后，随机变量Y的不确定性。

#### 信息熵增益

可以看到，我们现在可以计算随机变量Y原本的不确定性，还可以计算引入特征X后的不确定性，那么我们简单的将两式子相减就可以得到引入特征X后，不确定性减少的程度，也就是信息熵增益。其表达式如下：
$$
g(Y,X)=H(Y)-H(Y|X)
$$

### 构建决策树

有了熵，条件熵，和信息熵增益的概念，构建决策树就是一件非常简单的事情了。首先计算原始熵，然后计算引入各个特征之后的条件熵，分别算出每个特征的信息熵增益，将增益最大的特征放在第一个判断结点。重复上述步骤，直到没有特征剩余，决策树就构建完成了，非常简单。

### 例子

现在各大社交网络上充斥着各种僵尸粉，我们能否通过构造决策树来判断一个账号是否真实存在呢？通过人工浏览，我们获取了十个账号的信息，包括日志密度，好友密度，是否使用真实头像并对其真实性进行了人工标注，信息如下：

![例题](http://ozaeyj71y.bkt.clouddn.com/image/decision_tree/H.jpg%E4%BE%8B%E9%A2%98.png)

其中s，m和l分别代表小，中，大。

设L、F、H和R表示日志密度、好友密度、是否使用真实头像和账号是否真实。下面计算各属性的信息增益。

首先计算原始熵：

$$
H(R)= -0.7 \times log(0.7)-0.3 \times log(0.3)=0.7\times0.51+0.3\times1.74=0.879
$$

然后计算引入日志密度后的条件熵：
$$
H(L|R)=0.3\times(-\frac{1}{3}log\frac{1}{3}-\frac{2}{3}log\frac{2}{3})+0.4\times(-\frac{1}{4}log\frac{1}{4}-\frac{3}{4}log\frac{3}{4})+
0.3\times(-\frac{0}{3}log\frac{0}{3}-\frac{3}{3}log\frac{3}{3})=0.602
$$
上式分为三部分，账号日志密度为s的概率为3/10=0.3，在此条件下，账号为真的概率为1/3，假的概率为2/3，可以得到上式第一部分。日志密度为l和m时同理。

计算日志密度的熵增益：
$$
gain(L,R)=H(R)-H(L|R)=0.879-0.602=0.277
$$
计算另一个特征好友密度的条件熵：
$$
\begin{align}
H(F|R)=0.4\times(-\frac{1}{4}log\frac{1}{4}-\frac{3}{4}log\frac{3}{4})+ \\ 0.4\times(-\frac{0}{4}log\frac{0}{4}-\frac{4}{4}log\frac{4}{4})+ \\
0.2\times(-\frac{0}{2}log\frac{0}{2}-\frac{2}{2}log\frac{2}{2})=0.325
\end{align}
$$
计算其熵增益：
$$
gain(F,R)=H(R)-H(F|R)=0.879-0.325=0.554
$$
计算最后一个特征真实头像的条件熵：
$$
H(H|R)=0.5\times(-\frac{2}{5}log\frac{2}{5}-\frac{3}{5}log\frac{3}{5})+0.5\times(-\frac{1}{5}log\frac{1}{5}-\frac{4}{5}log\frac{4}{5})=0.846
$$
计算其熵增益：
$$
gain(H,R)=H(R)-H(H|R)=0.879-0.846=0.023
$$
可以看到，三个熵增益中，好友密度的熵增益最大，因此将其放在第一个判断结点。之后重复上面的三个步骤，确定后两个特征的顺序，即可构造出整棵决策树。

### 结语

写到这里，决策树的基本思想就已经全部介绍完毕了。但是这并不是决策树的所有内容，按照上述方法构造出来的决策树还有很多的提升空间，比如说剪枝来预防过拟合问题，或者引入信息增益比的概念来解决信息增益准则对于可取值数目较多的属性有所偏好的问题。这都属于决策树的进阶技巧了，我将另开一篇文章对其进行通俗易懂的解释。对于最基本的决策树，我想说上述计算过程其实是一种极大似然估计，至于什么是极大似然估计让我们在下一篇文章《线性回归与逻辑回归》中见。

最后，本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。

### Reference

CAWI2016-行为识别课程2-机器学习-决策树分类PPT

算法杂货铺——分类算法之决策树（Decision Tree）

数学之美


