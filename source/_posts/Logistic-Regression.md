---
title: 逻辑回归(Logistic_Regression)
date: 2017-11-23 00:07:28
tags: 机器学习-原理讲解
categories: 学习心得
mathjax: true
---

### 逻辑回归的由来

线性回归能够做到具体值的预测，但实际生活中大家也有很多分类需求，比如说西瓜甜不甜，IPhoneX值不值得买，网线那头使劲卖萌的网友是萌妹还是大屌萌妹等。这时候线性回归就显得不那么灵活了。那么我们能不能通过稍微改进一下线性回归，使他可以进行分类呢？要做到这件事，首先需要一个函数把线性方程的值域映射到(0,1)。这样做有一个好处，就是函数的结果可以用来表示分类到1的概率，比如说计算结果是0.6，那就是说分类到1的概率为60%。那么这个函数最好可以在0.5附近变化最快，这样我们更不容易得到0.5附近的结果，使得看上去更可靠。最终人们找到了一个符合上述要求的函数——逻辑斯蒂函数(Logistic Function)，这大概就是逻辑回归的由来吧。

### 逻辑斯蒂函数(Logistic Function)

该函数也叫Sigmoid函数，在神经网络中会再次提到。他的表达式如下：
$$
S(x)=\frac {1}{1+e^{-x}}
$$
他的图像如下：

![600px-Logistic-curve.svg](http://ozaeyj71y.bkt.clouddn.com/image/LR/600px-Logistic-curve.svg.png)

可以看到满足上述性质。除此之外他还有一点好处，求导很容易：
$$
S^{'}(x)=\frac {e^{-x}}{(1+e^{-x})^2}=\frac {1}{1+e^{-x}}(1-\frac {1}{1+e^{-x}})=S(x)(1-S(x))
$$

### 逻辑回归

和线性回归一样，逻辑回归也是想找到一组最优的参数值，我们先写出逻辑回归的 $h\_\theta(x)$ ：
$$
h\_\theta(x)=S(\theta^Tx)=\frac {1}{1+e^{-\theta^Tx}}
$$
这个方程通过差的平方和来构造代价函数来求最小值是不太好用的，因为那是一个非凸函数，存在多个极值点，无论是使用梯度下降还是最小二乘法都有可能陷入局部最有解。因此我们使用极大似然估计来寻找那一组最优的参数值。那么首先我们需要表示出在参数$\theta$和输入x确定的条件下，y出现的概率。根据我们上文说的函数的结果可以用来表示分类到1的概率，我们可以写出下面两个式子：
$$
P(y=1|x;\theta)=h\_\theta(x) \\\
P(y=0|x;\theta)=1-h\_\theta(x)
$$
我们可以用一个小技巧将其合并在一起：
$$
p(y|x;\theta)=(h\_\theta(x))^y(1-h\_\theta(x))^{1-y}
$$
看起来很复杂，其实很简单。$y=1$时，后一半没了，得到式子1，$y=0$时，前一半没了得到式子2。

根据极大似然估计的思想，我们想要在参数$\theta$和输入x确定的条件下，所有样本的真实值y出现的概率越大越好，那么我们需要求一个联合概率，因为他们相互独立所以我们可以把所有y出现的概率连乘，得到likelihood函数：
$$
L(\theta)= \prod\_{i=1}^{m}(h\_\theta(x)^y(1-h\_\theta(x))^{1-y}
$$
我们需要求上式的最大值，连乘的形式很难求，用$\log$函数转化为累加就比较好解决了：
$$
\begin{align}
l(\theta)&=\log L(\theta) \\\
&=\sum\_{i=1}^{m}y^{(i)}\log h\_\theta(x^{(i)})+(1-y^{(i)})\log (1-h\_\theta(x^{(i)}))
\end{align}
$$
上式是一个凸函数，极值点就是最值点，而且可导，因此使用极大似然估计是优于差的平方和构造代价函数的。

我们求其对每一个$\theta$的偏导：

$$
\begin{align}
\frac {\partial}{\partial\theta\_j}l(\theta) &=\sum\_{i=1}^{m}(y^{(i)}\frac 1{ h\_\theta(x^{(i)})}-(1-y^{(i)})\frac 1{ (1-h\_\theta(x^{(i)}))})\frac {\partial}{\partial\theta\_j}h\_\theta(x^{(i)}) \\\
&= \sum\_{i=1}^{m}(y^{(i)}\frac 1{ h\_\theta(x^{(i)})}-(1-y^{(i)})\frac 1{ (1-h\_\theta(x^{(i)}))})h\_\theta(x^{(i)})(1-h\_\theta(x^{(i)}))\frac {\partial}{\partial\theta\_j}\theta^Tx^{(i)} \\\
&= \sum\_{i=1}^{m}(y^{(i)}(1-h\_\theta(x^{(i)}))-(1-y^{(i)})h\_\theta(x^{(i)}))x\_j^{(i)}\\\
&=  \sum\_{i=1}^{m}(y^{(i)}-h\_\theta(x^{(i)}))x\_j^{(i)}
\end{align}
$$

上面这个式子只是用了几次求导中的链式求导法则而已，属于高中数学，注意下 

$$
h\_\theta(x)=S(\theta^Tx)=\frac {1}{1+e^{-\theta^Tx}}
$$ 

之后一步一步看，很容易看明白。

因为这里我们是求最大值，所以要用梯度上升而不是下降，每个$\theta$的更新法则如下：
$$
\theta\_j:=\theta\_j+\alpha \frac {\partial}{\partial\theta\_j}l(\theta)=\theta\_j+\alpha\sum\_{i=1}^{m}(y^{(i)}-h\_\theta(x^{(i)}))x^{(i)}\_j
$$
重复上式，直到收敛，即可求得最大值，而使其取得最大值的$\theta$就是我们找的那一组最优的参数。

### 总结

至此，逻辑回归的基本思想已经解释完毕。其实就是线性回归套上了一个逻辑斯蒂函数，把值域映射到了(0,1)之间而已。大部分的知识比如说极大似然估计，梯度上升法，都在线性回归中介绍过了。如果你看不懂这两个，可以去线性回归2和线性回归3中找答案。

最后，本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。

### Reference

斯坦福机器学习公开课讲义note1

知乎-逻辑回归损失函数为什么使用最大似然估计而不用最小二乘法 zzanswer的回答