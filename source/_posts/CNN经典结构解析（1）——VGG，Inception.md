---
title:  CNN经典结构解析（1）——VGG，Inception
date: 2018-12-21 23:33:28
tags: 深入理解深度学习算法
categories: 学习心得
mathjax: true
---


### 前言

上一篇讲了CNN的基本结构，主要分析了CNN的设计思路，以及各个基本零部件的功能。入门CNN之后，迎面而来的就是各种经典结构，LeNet, AlexNet, VGG, GoogLeNet, ResNet…… CNN每个基本模块像不同的乐高积木，这些经典结构就好像是个中高手设计的搭建图纸一样，照着他们的图纸搭，拼出来的东西一定不差。

但是，高手为什么要这样设计结构呢？综合看来，设计思路可以大致分为两类：

1. 选择多大的卷积核
2. 如何连接卷积层

本文主要分析VGG和GoogLeNet，这两个结构主要就在解决第一个问题，选择多大的卷积核。

### 绕不开的LeNet和AlexNet

提到卷积神经网络（CNN），就一定绕不开其开山鼻祖LeNet，以及在ImageNet 2012上大放异彩的AlexNet。

早在1998年，Yann Lecun就提出了用于手写体数字识别的LeNet。那时候的LeNet就已经包含了CNN的全部基本组件：卷积层，池化层，全连接层。但是受限于当时的计算能力，以及数据稀缺等原因，LeNet并没有受到广泛的关注。

AlexNet在ImageNet 2012上大放异彩，把问题的错误率从25%降低了到了15%。这种夸张的提升，使得CNN受到了广泛的关注。AlexNet并没有在结构上做出很多调整，在我阅读AlexNet的原文时，我感到作者更多的是在讲如何将这样一个在当时来说比较大的模型，成功运行的。

这里，我不想具体分析LeNet以及AlexNet的具体结构。我只想强调一点，他们用的卷积核大小是不一样的。LeNet中的卷积层采用的都是5x5的卷积核，而在AlexNet中用的是11x11，5x5以及3x3的卷积核。那么卷积核的大小对CNN来说意味着什么，到底选择多大的卷积核是最优解，就成了大家必须考虑的问题。

### VGG

卷积核大小对应的实际上是“感受野”。比如说一个3x3的卷积核它只与矩阵中3x3范围内的数字进行卷积运算，所以它的感受野比较的小。而7x7和11x11则可以与更大范围内的数字进行运行，他们的感受野更大。

VGG的亮点在于，所有的卷积核全都使用3x3大小（极少的采用了1x1）。原因是，两个连续的3x3卷积层（中间没有下采样层）具有的感受野和一个5x5的卷积层相当。图示：

![VGG](C:\Users\Magina507\Desktop\VGG.jpg)

可以看到一个5x5大小的图片，经过两次3x3卷积后，输出为1x1，这和它经过一个5x5卷积的输出维度是一样的，也就是说他们的感受野是相同的。

同理7x7的卷积可以由3个3x3的卷积代替，以此类推，更大的卷积核都可以分解成几个3x3的卷积核来进行代替。这就是VGG把卷积核全都换成3x3的原因。

这样做还有两个好处：

1. 更多的层数代表经过了更多次的激活函数，增强了模型的非线性能力，这意味着模型的分类能力更强。
2. 减少了模型的参数。以7x7卷积为例，3个3x3卷积一共是27个参数，7x7卷积需要49个参数，比27多了81%。

也就是说使用N个3x3来代替其他尺寸的卷积核，不仅在感受野上可以替代掉其他尺寸的卷积核，还能用更少的参数获得更多的性能。这就是VGG结构的优势。

### GoogLeNet

GoogLeNet 最大的特点是使用了 Inception 模块，然而Inception模块有非常多的版本（Inception V1~V4 以及 Inception-ResNet），每一个版本改良的目标都是想用更少的计算资源获得更优的结果，但引入的Trick也越来越多，如果对整个Inception的发展历史感兴趣的话，推荐阅读机器之心的《一文概览Inception家族的「奋斗史」》，本文将只会涉及Inception V1。

与VGG的全都3x3不同，GoogLeNet的思路是：“小孩才做选择，大人全都要”。所以他干脆弄了一个Inception模块出来，里面放入了当时最流行的几种卷积/池化层，然后通过concatenation操作，把所有输出结果拼一起。

![1545475485(1)](C:\Users\Magina507\Desktop\1545475485(1).jpg)

既然我不知道选哪个好，那就全要，然后拼一起，通过训练让模型自己去对卷积核进行投票。然而这样做有一个显而易见的缺点->消耗大量的计算资源。一个Inception模块顶的上别人四层，而且每个卷积的结果是直接拼一起的，就导致维度会越来越大，所以他们又引入了1x1卷积来控制维度。

![1545475976(1)](C:\Users\Magina507\Desktop\1545475976(1).jpg)

这就是Inception V1的设计思想。

后面各种版本的改良主要是为了减少模型中的参数，比如把5x5用两个3x3表示，或者把NxN的卷积核分成Nx1以及1xN等。这些操作已经和本文主要想阐明的“选多大的卷积核”关系不大了，不再赘述。

### 总结

1. LeNet是CNN的开山之作，AlexNet作为其更深更宽的版本，向世人展示了CNN的强大之处。
2. VGG认为卷积核全用3x3的就好，因为多个3x3是参数更少，性能更好的卷积核。
3. GoogLeNet认为既然不知道拿个会比较好，我就全都要。然后再通过各种各样的操作来减少模型中的参数，降低运算复杂度。

最后，设计思路的总结（选多大的卷积核，怎么连接）只是我的一家之言，理解不到位的地方欢迎在评论区指正。