<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="你自由的样子，他们说你像疯子">
<meta property="og:type" content="website">
<meta property="og:title" content="Magina的个人小站">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Magina的个人小站">
<meta property="og:description" content="你自由的样子，他们说你像疯子">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Magina的个人小站">
<meta name="twitter:description" content="你自由的样子，他们说你像疯子">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Magina的个人小站</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?27b0304965fa090d10656dd146089b41";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Magina的个人小站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/21/RL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/21/RL/" itemprop="url">强化学习超概括总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-21T16:20:26+09:00">
                2018-04-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/知识总结/" itemprop="url" rel="index">
                    <span itemprop="name">知识总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Udacity深度学习的课程已经学到了最后，本来以为很快就可以做完最后一个项目完成课程。结果强化学习的内容比想象的要多的多，也深的多，花费了很长时间也没完成项目。本文将会概括我近期的学习内容，总结出强化学习的知识框架，以帮助我进一步学习。</p>
<h3 id="强化学习要解决什么问题？"><a href="#强化学习要解决什么问题？" class="headerlink" title="强化学习要解决什么问题？"></a>强化学习要解决什么问题？</h3><p>一句话总结，强化学习要解决的问题和模糊系统（Fuzzy System）类似，都是复杂环境下的控制问题。</p>
<p>机器和人最大的不同在于人不需要知道知道具体的数值就可以做出动作或者决策，而机器不行。比如说走路，如果要让一个机器走路，实际上就是控制机器人各个关节施加的力，力的大小方向时机搭配合适了，机器人就能成功的走路了，但换一个路面这个搭配可能就不对了，需要重新调整。而人不需要这样，每个人都会走路，然而我们并不知道施加了多少力在关节上，甚至走路主要用到的是哪块肌肉都说不出来，但是人可以走各种各样的路。</p>
<p>有没有一种办法可以让机器人也能用人类的这种方式来做事，就是强化学习要解决的问题。</p>
<h3 id="怎么解决？"><a href="#怎么解决？" class="headerlink" title="怎么解决？"></a>怎么解决？</h3><p>首先是把转化问题。在走路问题中，评价力的组合是好还是坏事困难的，</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/21/CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/21/CNN/" itemprop="url">深入理解卷积神经网络(CNN)——基础理论</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-21T01:33:00+09:00">
                2018-03-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习心得/" itemprop="url" rel="index">
                    <span itemprop="name">学习心得</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>深度学习现在这么火热的一大原因就是卷积神经网络（以下简称CNN）在2012的ImageNet竞赛上取得了令人惊讶的进步，准确率提升了10%。所以专栏第一篇我想谈谈卷积神经网络。<br>在学习卷积神经网络的过程中，我发现大部分资料集中在什么是卷积层，什么是Pooling层，怎么计算每层的输出尺寸，卷积层和Pooling层以什么形式叠加，这些问题上。这些资料虽然带我入了CNN的门，然而等我回过神来，我却发现我连为什么要使用CNN都答不上来。同时我产生了更多疑问，比如为什么要使用卷积层？为什么要使用Pooling层？为什么要这样堆叠不同的层？单一个因为他work是不能让我通体舒畅的。<br>本文的目标是介绍CNN的基本结构，并在这个过程中穿插着我对上述问题的回答。我将从以下几个部分展开全文：</p>
<ol>
<li>卷积神经网络在图像分类问题上有什么优势？</li>
<li>为什么要使用卷积层？</li>
<li>为什么要使用池化层？</li>
<li>全连接层的作用</li>
<li>总结</li>
</ol>
<p>PS：以下的讨论都是建立在一个已经训练好的CNN模型上的，本文不会讨论如何训练CNN网络，也不会分析CNN下的BP算法，比起这些如何理解CNN更加重要。</p>
<h3 id="1-CNN在图像分类问题上有什么优势？"><a href="#1-CNN在图像分类问题上有什么优势？" class="headerlink" title="1. CNN在图像分类问题上有什么优势？"></a>1. CNN在图像分类问题上有什么优势？</h3><p>现存问题和新方法的出现是一对孪生兄弟。在想不通CNN有什么优势的时候，把目光放在了之前研究的劣势上，无疑可以帮助我们缕清思绪。这里用水果分类来分析一下SVM以及神经网络的劣势。<br>如果我们有一组水果的图片，里面有草莓，香蕉和橘子。在图片尺寸较大的情况下，使用SVM分类的步骤是</p>
<ol>
<li>人工提取特征，比如说大小，形状，重量，颜色等。</li>
<li>根据上述特征，把每一张图片映射到空间中的一个点，空间的维度和特征的数量相等。</li>
<li>相同类别的物体具有类似的特征，所以空间中标记为草莓的点肯定是聚在一起的，香蕉和橘子也是同理。这时候使用SVM算法在空间中划出各类点之间的分界线就完成了分类。</li>
</ol>
<p>在最后一步中，不使用SVM，用别的分类器也是可以的，比如KNN，贝叶斯，甚至神经网络都是可以的。虽然不同算法中性能会有差异，但是这里我想说的是在图像分类问题上的瓶颈并不在算法的性能上，而是在特征的提取上。<br>区分草莓和橘子的特征是容易提取的，那橘子和橙子呢？如果上述四个特征不能很好的区分橘子和橙子，想要进一步提升算法的性能怎么办？通常的做法是需要提取新的特征。那么新特征应该如何选择呢？对于我这种水果盲来说，这个问题是具有一定难度的。<br>除了橘子橙子问题，我们还有猫狗如何区分，狗品种如何识别等一系列问题。我想对于大部分人来说，狗狗品种的识别是非常有难度的。转了一圈回来，突然发现，图像分类任务的瓶颈竟然出现在特征选择上。（诚然目前有SIFT、HOG、LBP、LDP等自动提取特征的算法，但是效果并不理想，有局限性。）<br>如果我们用神经网络直接对猫狗进行分类呢？这样不就避开了特征提取这一步了吗？假设输入图片大小为30×30，那么设置900个输入神经元，隐含层设置1000个神经元，输出神经元个数对应需要的输出数量不久好了吗？甚至用SVM也可以这样做，把一张30×30的图看作900维空间中的一个点，代表猫的点和代表狗的点在这个900维的空间中必然是相聚于两个簇，然后我们就又可以使用SVM来划出分界线了。<br>但是这样计算开销就太大了，对于30×30的图片我们也许可以这样做，对于1000×1000的图片我们这样做的话就需要至少一百万个隐层神经元，这样我们就至少要更新 …emmmm…10^12个参数。而SVM的话，则相当于在一百万维的空间中运行了。运算量将会大的难以估计。另外，图片中并不是所有的信息都和是我们需要的。背景对我们的分类毫无价值，然而在这种一股脑全部拿来做输入的情况下，背景也被当成了特征进入了模型当中，准确度自然会有所下降。<br>总之，如果不人工提取特征，那么计算量会非常大，精确度也无法保证。而人工提取特征的方式又会在某些问题下难以进行，比如狗狗品种分类。<br>而CNN通过他独有的方式，成功解决了这两个问题。也就是说，CNN是一个可以自动提取特征，而且待训练参数相对不那么多的神经网络，这就是CNN在图像分类任务中的决定性优势。</p>
<h3 id="2-为什么要使用卷积层？"><a href="#2-为什么要使用卷积层？" class="headerlink" title="2.为什么要使用卷积层？"></a>2.为什么要使用卷积层？</h3><p>和神经网络模型类似，CNN的设计灵感同样来自于对神经细胞的研究。<br>1981 年的诺贝尔医学奖，颁发给了 David Hubel、TorstenWiesel，以及 Roger Sperry。他们的主要贡献，是发现了人的视觉系统的信息处理是分级的。</p>
<blockquote>
<p>从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。</p>
</blockquote>
<p>值得注意的是，最低级的V1区需要提取边缘特征，而在上面提到的分类中，神经网络实际上是把30×30的图片按照900个像素点处理的。那么有没有一种方法能够让神经网络像人一样，按照边缘来理解呢？有的，这个方法就是卷积。<br>卷积计算并不复杂，矩阵对应元素相乘的和就是卷积的结果，到了神经网络中会多出偏置b还有激活函数，具体方法如下图：<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/CNN/CNN1.png" alt="图片1"><br>图片源于Udacity深度学习纳米学位课程<br>图片展示的是由九个权重组成的矩阵 $\left( \begin{array}{ccc}-1&amp;-1&amp;1\\ -1&amp;1&amp;-1\\1&amp;-1&amp;-1 \\ \end{array}\right)$ 和图片上九个像素点组成矩阵 $\left( \begin{array}{ccc} 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ \end{array} \right)$ 进行卷积过程。在偏置b为0，激活函数使用ReLU的情况下，过程就像图片右下角的公式一样，对应元素乘积的和，再加上值为0的b，然后外套激活函数得到输出0。<br>你可能会想这部分的计算和普通的神经网络没什么差别，形式都是$f(wx+b)$。那么这么处理和边缘有什么关系？多做几次卷积就知道了。<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/CNN/CNN2.png" alt="图片2"><br>图片源于Udacity深度学习纳米学位课程<br>Filter指的是权重组成矩阵，Input Layer中存的是图片中的全部像素。Convolutional Layer存的是Filter与图片中所有3×3矩阵依次卷积后得到的结果。在输出中我们可以看到两个三，他们比其他的元素0都要大，是什么决定了卷积结果的大小？观察后发现，图中参与卷积的部分1的排列和活动窗口中1的排列完全一样时，输出为3。而像素的排列方式其实就是图片中的形状，这说明如果图像中的形状和Filter中的形状相似的话，输出值就大，不像就小。因此，卷积的操作建立了神经网络与图像边缘的联系。<br>实际上CNN经过训练之后，Filter中就是图片的边缘啊，角落之类的特征。也就是说，卷积层是在自动提取图片中的特征。<br>除此之外，卷积还有一种局部连接的思想在里面。它对图片的处理方式是一块一块的，并不是所有像素值一起处理，因此可以极大的降低参数值的总量。这里我需要使用那张经典的图片来说明卷积层是如何降低参数总量的：<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/CNN/CNN3.jpg" alt="图片3"><br>图片源于百度图片<br>对于一张1000×1000的图片来说，对他进行分类，至少需要10的12次方个参数。而如果对图片使用卷积操作，每个神经元只和图像上的10×10的像素连接的话，参数总量就变成了10的8次方。但是这样的操作会导致一个问题，每个神经元只对应图片一部分的内容，那么这个神经元学到的内容就不能应用到其他神经元上。比如说有这样一个训练集，同样姿态的猫出现在黑色神经元负责的区域中，但是测试集中，猫可能出现在图片的任何位置。按照局部链接的做法，其他区域的猫是无法被正确识别的。<br>而为了让出现在任何位置的猫都能够被正确识别，提出了权重共享。让红绿蓝黑神经元中的参数全都一样，这样就可以使得模型的准确率不受物体位置的影响，看起来就像同一个Filter滑过了整个图片。从提取特征的角度上来讲，一个Filter显然不能满足需求，因此需要生成多个不同的Filter来对图片进行卷积。更棒的是，为了获得平移不变性而使用的权重共享法，又意外的再一次降低了待训练参数总数。<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/CNN/CNN4.jpg" alt="图片4"><br>就算使用100个权值共享的10×10Filter来卷积，总参数也才10的4次方。也就是说，参数相较于普通的神经网络而言，总共下降了整整8个数量级，这种提升是夸张的。<br>写到这里，卷积层的工作方式就已经全部出来了。具体工作流程如下图：<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/CNN/%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97.gif" alt="图片5"><br>图片源于百度图片<br>蓝色的部分代表输入图像；绿色的部分代表输出矩阵；周围的虚线是padding操作，可以看作图像的一部分；下方不断移动的阴影就是Filter，其大小，数量，一次移动的距离都是可以自定义的；阴影至上方绿色的连线代表相乘再相加之后的结果输出到了输出矩阵的哪个位置。卷积层的这种操作方式，成功的模拟了生物视觉系统中的边缘特征提取部分。<br>而CNN中对于分级结构的模拟，是通过卷积层的层层叠加实现的。AlexNet的论文中不止一次的提到，网络的深度对CNN性能的影响是显著的。可以认为卷积层的不断叠加会使得提取到的特征越来越复杂，整个流程就像上述引用中提到的人类的视觉系统的工作方式一样运行，最终完成对图片的分类。<br>那么现在就可以很轻松的回答标题的问题了，使用卷积层是因为卷积层本质上是在自动提取图片的特征，而且在提取特征的同时，极大的降低了网络总体待训参数的总量。这两个特性使得CNN克服了特征提取困难，待训参数庞大的问题，成功制霸图片分类问题。</p>
<h3 id="3-为什么要使用池化层"><a href="#3-为什么要使用池化层" class="headerlink" title="3.为什么要使用池化层"></a>3.为什么要使用池化层</h3><p>你可能会问，卷积层就已经把参数降了下来，还解决了特征提取的问题，那还加一个池化层干什么呢？我认为，池化层只是工程上想让网络更深而做出的一个无奈之举。<br>就以最经常出现的最大池化为例，来看看所谓的池化操作有多么随便吧。<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/CNN/CNN5.png" alt="图片6"><br>一个2×2的最大池化操作如上图，他做的就是把2×2窗口中的最大值存下来。所以绿色部分留下来了6；棕黄色部分是8；棕红色部分是3；蓝色部分是4。对，就是这么简单，就是这么随便。这个操作一眼看上去优点我没想出来，但是缺点却显而易见——损失了细节。为什么损失细节也要做这一步呢？我能想到的唯一的原因是要压缩矩阵，这样可以在模型中多加几层卷积层，用来提取更高维，更复杂的特征。而从压缩的角度上来看，这一步可谓简单有效，一个取最大值的操作，就让矩阵大小变为四分之一。<br>AlexNet的出现是2012年，那时候用的是GTX580，3G显存，文章中提到只用一块GPU是不行的，因为显存会爆，因此用了两块GPU并行进行的这个任务。想必其作者也是苦于总是爆显存，而不得已加上的池化层。就算这样，还要用到两块GPU才成功训练了整个网络。<br>然而池化层的应用似乎带来了更多的便利之处。由于其只取最大值，忽视掉了其他影响较小的值，所以在当内容发生很小的变化的时候包括一些平移旋转，CNN 仍然能够稳定识别对应内容。也就说池化层给模型带来了一定程度上的不变性。<br>而应不应该使用池化层还是一个正在讨论的问题，有的网络用，有的网络不用。按照我的理解，在显存够用的情况下，不用池化层。这种丢失细节提升模型不变性的方法有一点七伤拳的意思。而且我们希望得到的模型并不是在不知道图片变化了的情况下可以得到正确的结果，我们希望的是模型可以认识到差异却依然能做出正确的分类才对。不过在以准确率为纲的今天，如果用池化层能提升准确率，就加上吧，无可厚非。</p>
<h3 id="4-全连接层的作用"><a href="#4-全连接层的作用" class="headerlink" title="4.全连接层的作用"></a>4.全连接层的作用</h3><p>在经过几次卷积和池化的操作之后，卷积神经网络的最后一步是全连接层。这一步就和最最普通的神经网络没有什么区别。我认为这里的神经网络就是充当一个分类器的作用，输入是不同特征的特征值，输出是分类。我甚至认为在训练好之后，把全连接层砍掉，把卷积部分的输出当作是特征，全连接换成SVM或者别的分类器，重新训练，也是可以取得良好效果的。</p>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h3><p>这里重新整理一下本文的内容：</p>
<ol>
<li>CNN之前的图片分类算法性能受制于特征的提取以及庞大参数数量导致的计算困难。</li>
<li>使用卷积来模拟人类视觉系统的工作方式，而这种方式极大的降低了神经网络的待训练参数数量。</li>
<li>为了获得平移不变性，使用了权重共享技术，该技术进一步降低了待训练参数数量。</li>
<li>卷积层实际上是在自动提取图片特征，解决了图像特征提取这一难题。</li>
<li>使用池化层的根本原因是降低计算量，而其带来的不变性并不是我们需要的。不过在以模型准确率为纲的大背景下，继续使用无可厚非。</li>
<li>全连接层实质上就是一个分类器。</li>
</ol>
<h3 id="6-参考"><a href="#6-参考" class="headerlink" title="6. 参考"></a>6. 参考</h3><p>主要参考Udactiy深度学习纳米学位卷积神经网络部分，以及CS231n的相关内容写成。有任何疑问都欢迎在评论区指出。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/30/dft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/30/dft/" itemprop="url">Use python to implement Discrete Fourier transform</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-30T01:58:05+09:00">
                2018-01-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Report/" itemprop="url" rel="index">
                    <span itemprop="name">Report</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>In image processing, Discrete Fourier Transformation is a very useful method. However, its formula is so complicated that it is difficult to understand. As a result, I intend to deepen my understanding of it by implementing it. Since I am not familiar with c or c++, I use python to do this task. During the task, I realize that the efficiency of my method is not as good as I thought, so I use several methods to improve it. The goal of my work is implementing the Discrete Fourier Transformation(DFT) in the most efficient way in python.</p>
<p>In this report, I would like to show my work in the following aspects:</p>
<ol>
<li>Introduction of Discrete Fourier Transformation.</li>
<li>Introduction of my experiment.</li>
<li>Implement the DFT with standard formula.</li>
<li>Implement the DFT with Euler’s formula.</li>
<li>Implement the DFT with matrix.</li>
<li>The principle of Fast Fourier Transform(FFT).</li>
<li>Conclusion.</li>
</ol>
<h3 id="1-Introduction-of-Discrete-Fourier-Transformation"><a href="#1-Introduction-of-Discrete-Fourier-Transformation" class="headerlink" title="1. Introduction of Discrete Fourier Transformation"></a>1. Introduction of Discrete Fourier Transformation</h3><p>In 1822, Fourier, a French scientist, pointed out that any periodic function can be expressed as a composition of sine and cosine in different frequencies. The posterity found that in some conditions, even non-periodic function can be expressed by sine and cosine. Following this idea, Fourier Transformation(FT) is produced.</p>
<p>In image processing, the image data is discrete value. Based on the Fourier Transformation, Later researchers invent the Discrete Fourier Transformation(DFT) to hold the discrete value. Although the formula of FT and DFT is different, the principle of them is same. Therefore, we can use DFT to convert the image to a combine with sine and cosine. Even better, we could use the Inverse DFT to convert it back to image. As a result, DFT is very important in image processing.</p>
<h3 id="2-Introduction-to-my-experiment"><a href="#2-Introduction-to-my-experiment" class="headerlink" title="2. Introduction to my experiment"></a>2. Introduction to my experiment</h3><h4 id="2-1-Purpose"><a href="#2-1-Purpose" class="headerlink" title="2.1 Purpose"></a>2.1 Purpose</h4><p>I aim to use python to implement the DFT in the most efficient way. In this report, I implement the DFT in different ways and I would give the comparison of them.</p>
<h4 id="2-2-Programming-environment"><a href="#2-2-Programming-environment" class="headerlink" title="2.2 Programming environment"></a>2.2 Programming environment</h4><p>The version of python is 3.6, IDE is jupyter notebook.</p>
<p>In my program, I use three libraries.</p>
<ol>
<li>Opencv. I use this library to read image from folder.</li>
<li>Numpy. I use this library to do matrix computing.</li>
<li>Matplotlib. I use this library to show my results in picture.</li>
</ol>
<p>(If researches have the same version python with libraries that I mentioned before, they can copy my code to jupyter notebook and run it to check my work.)</p>
<h4 id="2-3-Data-preparation"><a href="#2-3-Data-preparation" class="headerlink" title="2.3 Data preparation"></a>2.3 Data preparation</h4><p>I prepare two different sizes of classic image Lena to test my program. One is the original image and its size is 512*512. The other is shrank  image and its size is 50*50. Because the original  image size is too large to do the test. Some of the algorithms will be too slow to get a result.</p>
<p>Before the experiment, I read images from my folder. Here is code and explanations. I use function imread to read image and plt to show my results. Variable lena saves the information of the original lena and lena50 saves the information of the small sizes lena image.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cv2</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">lena = cv2.imread(<span class="string">"C:\lena.jpg"</span>,<span class="number">0</span>)   <span class="comment"># read image</span></div><div class="line">lena50 = cv2.imread(<span class="string">"C:\lena50.jpg"</span>,<span class="number">0</span>)</div><div class="line">plt.subplot(<span class="number">121</span>),plt.imshow(lena,<span class="string">'gray'</span>),plt.title(<span class="string">'lena_512*512'</span>)</div><div class="line">plt.subplot(<span class="number">122</span>),plt.imshow(lena50,<span class="string">'gray'</span>),plt.title(<span class="string">'lena_50*50'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/dft/output_3_0.png" alt="png"></p>
<h4 id="2-4-The-steps-of-evaluating-the-result-of-method"><a href="#2-4-The-steps-of-evaluating-the-result-of-method" class="headerlink" title="2.4 The steps of evaluating the result of method."></a>2.4 The steps of evaluating the result of method.</h4><p>I evaluate my result by following steps:</p>
<ol>
<li>Get result from my function.</li>
<li>Get the standard answer from numpy’s fft funtion.</li>
<li>Print out the result of each function.</li>
<li>Check the picture to see whether they are same or not.</li>
<li>Use allclose to check the result of my function whether correct or not.</li>
<li>For each function, use timeit function to calculate the cost of time.</li>
</ol>
<h3 id="3-Implement-the-DFT-with-standard-formula"><a href="#3-Implement-the-DFT-with-standard-formula" class="headerlink" title="3. Implement the DFT with standard formula."></a>3. Implement the DFT with standard formula.</h3><h4 id="3-1-How-to-implement-it"><a href="#3-1-How-to-implement-it" class="headerlink" title="3.1 How to implement it."></a>3.1 How to implement it.</h4><p>The standard formula of the DFT is:<br>$$<br>F(u,v)= \sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)e^{-j2\pi(ux/M+vy/N)}<br>$$<br>$f(x,y)$ means the pixel value. $M$ and $N$ is the length and width of the image$f(x,y)$.</p>
<p>We can get several information from this formula:</p>
<ol>
<li>The output of Fourier Transformation is a complex matrix.</li>
<li>Each pixel in the output is the sum of input pixel multiply a complicated formula.</li>
</ol>
<p>Based on these information, I start coding.<br>First, I use the shape function get the row and column information from input image.<br>Second, I build a complex matrix with same dimension of the input image.<br>Finally, I use four loops to implement the Fourier Transformation. Here is the code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dft</span><span class="params">(input_img)</span>:</span></div><div class="line">    rows = input_img.shape[<span class="number">0</span>]</div><div class="line">    cols = input_img.shape[<span class="number">1</span>]</div><div class="line">    output_img = np.zeros((rows,cols),complex)</div><div class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">0</span>,rows):</div><div class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">0</span>,cols):</div><div class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,rows):</div><div class="line">                <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">0</span>,cols):</div><div class="line">                    output_img[m][n] += input_img[x][y] * np.exp(<span class="number">-1j</span>*<span class="number">2</span>*math.pi*(m*x/rows+n*y/cols))</div><div class="line">    <span class="keyword">return</span> output_img</div></pre></td></tr></table></figure>
<h4 id="3-2-Result"><a href="#3-2-Result" class="headerlink" title="3.2 Result"></a>3.2 Result</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">dft_lena50 = dft(lena50)</div><div class="line">out_dft = np.log(np.abs(dft_lena50))</div><div class="line">fft_lena50 = np.fft.fft2(lena50)</div><div class="line">out_fft = np.log(np.abs(fft_lena50))</div><div class="line"></div><div class="line">plt.subplot(<span class="number">121</span>),plt.imshow(out_dft,<span class="string">'gray'</span>),plt.title(<span class="string">'dft_output'</span>)</div><div class="line">plt.subplot(<span class="number">122</span>),plt.imshow(out_fft,<span class="string">'gray'</span>),plt.title(<span class="string">'np.fft_output'</span>)</div><div class="line">plt.show()</div><div class="line">np.allclose(dft_lena50,fft_lena50)</div></pre></td></tr></table></figure>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/dft/output_8_0.png" alt="png"></p>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">%timeit dft_lena50 = dft(lena50)</div><div class="line">%timeit fft_lena50 = np.fft.fft2(lena50)</div></pre></td></tr></table></figure>
<pre><code>26.3 s ± 574 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
96.1 µs ± 1.98 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre><h4 id="3-3-Result-analysis"><a href="#3-3-Result-analysis" class="headerlink" title="3.3 Result analysis"></a>3.3 Result analysis</h4><p>We can see that the output image of dft is as same as the np.fft_output. And the result of np.allclose is true which means that each value in matrix dft_lena50 and fft_lena50 is equal. Therefore, I have already implemented the DFT. </p>
<p>However,my method cost so much time. At the beginning, I use original image(size 512*512) to do the test and I even can not get the result. After changing the size of it, I can get result. It is still too slow. By using my function, it costs 26.3 seconds in average. But by using the function in numpy, it costs only 96.1 us in average. As a result, I figure out two ways to improve my code. First, I think that the exponential calculation in the loop will consume a lot of computing resources, so I use Euler’s formula to convert the exponent calculation into addition calculation.</p>
<h3 id="4-Implement-the-DFT-with-Euler’s-formula"><a href="#4-Implement-the-DFT-with-Euler’s-formula" class="headerlink" title="4. Implement the DFT with Euler’s formula."></a>4. Implement the DFT with Euler’s formula.</h3><h4 id="4-1-How-to-implement-it"><a href="#4-1-How-to-implement-it" class="headerlink" title="4.1 How to implement it."></a>4.1 How to implement it.</h4><p>I use Euler’s formula to change the exponent calculation into addition calculation. Than I get a new formula:<br>$$<br>F(u,v)= \sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)*(\cos(-2\pi(ux/M+vy/N))+j\sin(-2\pi(ux/M+vy/N)))<br>$$<br>Implement is very simple. I just change one row in dft then I get dft_ol function. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dft_ol</span><span class="params">(input_img)</span>:</span></div><div class="line">    rows = input_img.shape[<span class="number">0</span>]</div><div class="line">    cols = input_img.shape[<span class="number">1</span>]</div><div class="line">    output_img = np.zeros((rows,cols),complex)</div><div class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">0</span>,rows):</div><div class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">0</span>,cols):</div><div class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,rows):</div><div class="line">                <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">0</span>,cols):</div><div class="line">                    w = <span class="number">-2</span>*math.pi*(m*x/rows+n*y/cols)</div><div class="line">                    output_img[m][n] += input_img[x][y] * (math.cos(w) + <span class="number">1j</span>*math.sin(w))</div><div class="line">    <span class="keyword">return</span> output_img</div></pre></td></tr></table></figure>
<h4 id="4-2-Result"><a href="#4-2-Result" class="headerlink" title="4.2 Result"></a>4.2 Result</h4><p>Then, I did the same evaluation of it. Here is the result:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">dftol_lena50 = dft_ol(lena50)</div><div class="line">out_dftol = np.log(np.abs(dftol_lena50))</div><div class="line">fft_lena50 = np.fft.fft2(lena50)</div><div class="line">out_fft = np.log(np.abs(fft_lena50))</div><div class="line"></div><div class="line">plt.subplot(<span class="number">121</span>),plt.imshow(out_dftol,<span class="string">'gray'</span>),plt.title(<span class="string">'dftol_output'</span>)</div><div class="line">plt.subplot(<span class="number">122</span>),plt.imshow(out_fft,<span class="string">'gray'</span>),plt.title(<span class="string">'np.fft_output'</span>)</div><div class="line">plt.show()</div><div class="line">np.allclose(dftol_lena50,fft_lena50)</div></pre></td></tr></table></figure>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/dft/output_15_0.png" alt="png"></p>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">%timeit dftol_lena50 = dft_ol(lena50)</div><div class="line">%timeit fft_lena50 = np.fft.fft2(lena50)</div></pre></td></tr></table></figure>
<pre><code>21.8 s ± 84 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
91.7 µs ± 855 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre><h4 id="4-3-Result-analysis"><a href="#4-3-Result-analysis" class="headerlink" title="4.3 Result analysis"></a>4.3 Result analysis</h4><p>We could see that the result is correct. Although the average time is a little bit shorter than before(21.8s in average), it is still not acceptable.</p>
<p>I think the problem is I used four loops in my code. In python, the efficiency of loop is low so I wander that is there any possibility to replace the loops? I check the formula again. The input of it is a matrix and the output of it is also a matrix. If we apply numpy library to do matrix computing, efficiency of calculating is high. Therefore I tried to write the formula in matrix way.</p>
<h3 id="5-Implement-the-DFT-with-matrix"><a href="#5-Implement-the-DFT-with-matrix" class="headerlink" title="5. Implement the DFT with matrix."></a>5. Implement the DFT with matrix.</h3><h4 id="5-1-How-to-implement-it"><a href="#5-1-How-to-implement-it" class="headerlink" title="5.1 How to implement it."></a>5.1 How to implement it.</h4><p>The image data is a two dimension matrix. Imply the DFT on two dimension data is a 2D-DFT problem.</p>
<p>The 2D-DFT can write in two 1D-DFT:<br>$$<br>F(u,v)=\sum_{x=0}^{M-1}e^{-j2\pi ux/M}\sum_{y=0}^{N-1}f(x,y)e^{-j2\pi vy/N}=\sum_{x=0}^{M-1}F(x,v)e^{-j2\pi ux/M}<br>$$</p>
<p>$$<br>F(x,v) =\sum_{y=0}^{N-1}f(x,y)e^{-j2\pi vy/N}<br>$$</p>
<p>and the 1D-DFT is easy to write in matrix way:<br>$$<br>F(x,v) = M\vec x<br>$$</p>
<p>$$<br>M = e^{-j2\pi \vec v \vec y/N}<br>$$</p>
<p>$\vec x $ means each row vectors of $f(x,y)$, $\vec v$ is a column vector $(0,1,2，\cdots，N-1)^T$,$\vec y$ is a row vector$(0,1,2，\cdots，N-1)$. </p>
<p>For 2D-Fourier Transformation , we just need to do the 1D-DFT for each row of input and do 1D-DFT for each column of the output from 1D-DFT for rows.</p>
<p>I follow this new formula building the dft_matrix function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dft_matrix</span><span class="params">(input_img)</span>:</span></div><div class="line">    rows = input_img.shape[<span class="number">0</span>]</div><div class="line">    cols = input_img.shape[<span class="number">1</span>]</div><div class="line">    t = np.zeros((rows,cols),complex)</div><div class="line">    output_img = np.zeros((rows,cols),complex)</div><div class="line">    m = np.arange(rows)</div><div class="line">    n = np.arange(cols)</div><div class="line">    x = m.reshape((rows,<span class="number">1</span>))</div><div class="line">    y = n.reshape((cols,<span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">0</span>,rows):</div><div class="line">        M1 = <span class="number">1j</span>*np.sin(<span class="number">-2</span>*np.pi*y*n/cols) + np.cos(<span class="number">-2</span>*np.pi*y*n/cols)</div><div class="line">        t[row] = np.dot(M1, input_img[row])</div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(<span class="number">0</span>,cols):</div><div class="line">        M2 = <span class="number">1j</span>*np.sin(<span class="number">-2</span>*np.pi*x*m/cols) + np.cos(<span class="number">-2</span>*np.pi*x*m/cols)</div><div class="line">        output_img[:,col] = np.dot(M2, t[:,col])</div><div class="line">    <span class="keyword">return</span> output_img</div></pre></td></tr></table></figure>
<h4 id="5-2-Result"><a href="#5-2-Result" class="headerlink" title="5.2 Result"></a>5.2 Result</h4><p>Also, I use the same flow to evaluate it. Here is the result:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">dftma_lena50 = dft_matrix(lena50)</div><div class="line">out_dftma = np.log(np.abs(dftma_lena50))</div><div class="line">fft_lena50 = np.fft.fft2(lena50)</div><div class="line">out_fftma = np.log(np.abs(fft_lena50))</div><div class="line"></div><div class="line">plt.subplot(<span class="number">121</span>),plt.imshow(out_dftma,<span class="string">'gray'</span>),plt.title(<span class="string">'dftma_output'</span>)</div><div class="line">plt.subplot(<span class="number">122</span>),plt.imshow(out_fftma,<span class="string">'gray'</span>),plt.title(<span class="string">'np.fft_output'</span>)</div><div class="line">plt.show()</div><div class="line">np.allclose(dftma_lena50,fft_lena50)</div></pre></td></tr></table></figure>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/dft/output_21_0.png" alt="png"></p>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">%timeit dftma_lena50 = dft_matrix(lena50)</div><div class="line">%timeit fft_lena50 = np.fft.fft2(lena50)</div></pre></td></tr></table></figure>
<pre><code>10.9 ms ± 196 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
92 µs ± 523 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre><p>The result is correct and the time is much shorter than other functions. It costs only 10.9ms in average. I think it is fast enough so I give the original lena as input and run it to see the result.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">dftma_lena = dft_matrix(lena)</div><div class="line">out_dftma = np.log(np.abs(dftma_lena))</div><div class="line">fft_lena = np.fft.fft2(lena)</div><div class="line">out_fftma = np.log(np.abs(fft_lena))</div><div class="line"></div><div class="line">plt.subplot(<span class="number">121</span>),plt.imshow(out_dftma,<span class="string">'gray'</span>),plt.title(<span class="string">'dftma_output'</span>)</div><div class="line">plt.subplot(<span class="number">122</span>),plt.imshow(out_fftma,<span class="string">'gray'</span>),plt.title(<span class="string">'np.fft_output'</span>)</div><div class="line">plt.show()</div><div class="line">np.allclose(dftma_lena,fft_lena)</div></pre></td></tr></table></figure>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/dft/output_24_0.png" alt="png"></p>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">%timeit dftma_lena = dft_matrix(lena)</div><div class="line">%timeit fft_lena = np.fft.fft2(lena)</div></pre></td></tr></table></figure>
<pre><code>16.3 s ± 448 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
11.3 ms ± 151 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><h4 id="5-3-Result-analysis"><a href="#5-3-Result-analysis" class="headerlink" title="5.3 Result analysis"></a>5.3 Result analysis</h4><p>The result is correct but the time of calculation increase rapidly as the size of image increases. Although np.fft cost more time than before, it is not increase so rapidly. I check this online, the method that numpy used is called Fast Fourier Transformation(FFT). Since this function is not in the lecture so I did not implement it in my report but I studied its principle. </p>
<h3 id="6-The-principle-of-Fast-Fourier-Transform-FFT"><a href="#6-The-principle-of-Fast-Fourier-Transform-FFT" class="headerlink" title="6. The principle of Fast Fourier Transform(FFT)."></a>6. The principle of Fast Fourier Transform(FFT).</h3><p>Since we can use two 1D-DFT to calculate the 2D-DFT, we only to improve the efficiency of 1D-DFT than we can improve the efficiency of 2D-DFT.</p>
<p>For a 1D-DFT:<br>$$<br>F(u)=\sum_{x=0}^{M-1}f(x)W_{M}^{ux}<br>$$<br>if M is divisible by 2, we can write it in two parts:<br>$$<br>M = 2K \<br>F(u)=\sum_{x=0}^{K-1}f(2x)W_{K}^{ux} + \sum_{x=0}^{M-1}f(2x+1)W_{K}^{ux}W_{2K}^{ux}<br>$$<br>But in this formation the length of $F(u)$ is only a half as before. We need to calculate the other part of it:<br>$$<br>F(u+K) = \sum_{x=0}^{K-1}f(2x)W_{K}^{ux} - \sum_{x=0}^{M-1}f(2x+1)W_{K}^{ux}W_{2K}^{ux}<br>$$<br>This is called the symmetry of DFT. We only calculate half of DFT than we can got the result of whole DFT as soon as M is divisible by 2.</p>
<p>Even better we can set $M=2^n$ , than we can divide the DFT into several small DFT and use the symmetry of DFT to reduce the amount of computation. This is the principle of FFT.</p>
<p>Time complexity of FFT is $O(nlogn)$ , DFT is $O(n^2)$. Therefore, it is much faster than the DFT when the n is large.</p>
<h3 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion."></a>7. Conclusion.</h3><p>Here is the result of all functions:</p>
<table>
<thead>
<tr>
<th>Function name</th>
<th>Input image</th>
<th>Answer correct or not</th>
<th>Time in average</th>
</tr>
</thead>
<tbody>
<tr>
<td>DFT with lecture’s formula</td>
<td>Lena50</td>
<td>True</td>
<td>26.3 seconds</td>
</tr>
<tr>
<td>DFT with Euler’s formula</td>
<td>Lena50</td>
<td>True</td>
<td>21.8 seconds</td>
</tr>
<tr>
<td>DFT with matrix</td>
<td>Lena50</td>
<td>True</td>
<td>10.9 ms</td>
</tr>
<tr>
<td>Numpy’s fft function</td>
<td>Lena50</td>
<td></td>
<td>91.7 us</td>
</tr>
<tr>
<td>DFT with matrix</td>
<td>Lena</td>
<td>True</td>
<td>16.3 seconds</td>
</tr>
<tr>
<td>Numpy’s fft function</td>
<td>Lena</td>
<td></td>
<td>11.3 ms</td>
</tr>
</tbody>
</table>
<p>As a result, I think the most efficient way to implement Discrete Fourier transform(DFT) in Python is use matrix to replace the loops. It is much faster than other method. However, the </p>
<p>After this report, I feel I understand the Discrete Fourier Transform deeper than before. Here are the conclusion of this task:</p>
<ol>
<li>The Fourier transform converts the image to a superposition of sine and cosine. It is not intuitive to imagine an image is a superposition of sine and cosine. However, if we treat all the pictures as an instant noodles, understanding the concept of Fourier Transformation will be much more easier. Images is an instant noodles, sine and cosine is the noodle in noodles. </li>
<li>The reason why we use Fourier transform is someone like thick noodle and others like the thin noodle. Fourier Transformation is a method that decomposition the instant noodles into one and one different noodle. Then we can choose the most appropriate noodle for different people. In image processing, it means that in some conditions, we may be interested in high frequency items and sometimes we may need the low frequency items. We can use the Fourier Transformation to find the desire items.</li>
<li>In python, it is very important to learn how to use an appropriate library. In this task, I use the matrix to replace the loops in function. I use the numpy library to do the matrix computing. With this help, I reduce the time from 21.8 seconds to 10.9ms. If I use the numpy’s FFT function directly, only cost 91.7us. Therefore, the efficiency of the functions in python library is very high. It is much more practical to find a corresponding library function when encounter a problem.</li>
<li>If we can’t find the corresponding library, it would be better to use others programming language to implement it such as C or C++. Because, without the help of library, python is too slow. </li>
</ol>
<h3 id="8-Reference"><a href="#8-Reference" class="headerlink" title="8. Reference"></a>8. Reference</h3><p>[1] Gonzalez, Rafael C, &amp; Woods, Richard E. (1977). Digital image processing. <strong>Prentice Hall International</strong>,<em> </em>28*(4), 484 - 486.</p>
<p>[2] 快速傅里叶变换（FFT）算法【详解】</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/06/NN-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/06/NN-1/" itemprop="url">深入理解神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-06T20:20:56+09:00">
                2017-12-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习心得/" itemprop="url" rel="index">
                    <span itemprop="name">学习心得</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>神经网络(Neural Network)是一种模仿生物神经网络构造的数学模型。很多文章喜欢从生物的神经元结构方面入手，展开对人工神经网络的介绍。但那样会引入更多意味不明的单词，比如突触，树突，轴突……所以这里并不会深入的介绍生物的神经网络是怎么工作的，只会说明为什么人工神经网络要构造成这样。本文按照时间顺序依次介绍神经网络中的几个重要概念，并在当中穿插一些思考，大致分为以下几个部分：</p>
<ol>
<li>MP神经元</li>
<li>感知机</li>
<li>多层前馈神经网络</li>
<li>误差逆传播算法（BP算法）</li>
<li>总结</li>
</ol>
<p>如果你对神经网络一无所知，本文会使得你对神经网络的来龙去脉有一些概念上的理解。如果你对神经网络有一定了解，本文的一些解释会让你对神经网络有豁然开朗的感觉。<br>PS: 本篇文章需要你对线性回归和逻辑回归有一定了解，如果你不懂，我也针对这两个问题专门写了几篇文章，链接为：<a href="https://wasedamagina.github.io/archives/。" target="_blank" rel="external">https://wasedamagina.github.io/archives/。</a></p>
<h3 id="1-MP神经元"><a href="#1-MP神经元" class="headerlink" title="1. MP神经元"></a>1. MP神经元</h3><p>MP神经元是神经网络的基本零部件，可以说神经网络就是神经元连接而成的网。理解神经网络，MP神经元是绕不开的。<br>MP神经元模型在1943年就提出了，这是一个非常古老而经典的神经元模型，直到今天，我们仍然在使用这个神经元模型。MP神经元是模仿生物的神经元设计的：<br>$x$ 模拟生物神经元中其他神经细胞给该细胞的刺激，值越大刺激越大；<br>$w$ 模拟该细胞不同来源的刺激的敏感度；<br>用阈值 $\theta$ 来描述激活该神经元的难易程度，越大越难激活；<br>用 $w1x1+w2x2+……wnxn-\theta$ 来计算神经元的兴奋程度；<br>$y=f(x)$ 为激活函数，用来计算神经元的输出；<br>在MP神经元中，激活函数为阶梯函数。兴奋函数大于阈值输出1，小于阈值输出0；<br>下图是MP神经元模型的示意图：<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/NN/MP%E7%A5%9E%E7%BB%8F%E5%85%83.png" alt="图片1"><br>图1.来源：《机器学习》——周志华</p>
<h3 id="2-感知机"><a href="#2-感知机" class="headerlink" title="2. 感知机"></a>2. 感知机</h3><p>由于MP神经元的输入来自于其他神经元，我们显然不能指望单独一个神经元就有什么功能。而多个神经元可以组成不同的结构，其中有一个叫做感知机。<br>感知机由输入和输出两层神经元组成，输入层有多个神经元，这些神经元只负责传递信号，没有任何其他功能；输出层只有一个MP神经元。这样的一个感知机可以用来做分类问题。<br>比如说它可以把 $(0,0),(0,1),(1,0),(1,1)$ 四个点分为两类。输入是一个坐标，所以输入神经元要有两个，这样的感知机结构图如下：<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/NN/%E6%84%9F%E7%9F%A5%E6%9C%BA.png" alt="图片2"><br>图2. 来源：《机器学习》——周志华<br>$w1=0.6,w2=0.6,\theta=1$ 的感知机只在输入是 $(1,1)$ 的时候激活。如果把0理解为假，1理解为真，那么这个感知机的功能就和与门是一样的，只有两个输入都是真的时候，输出真。而图像上，这个感知机实际上是一条线，把 $(1,1)$ 分在一边，其他点分在另一边。<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/NN/%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E6%84%9F%E7%9F%A5%E6%9C%BA.png" alt="图片3"><br>图3. 来源：《机器学习》——周志华<br>感知机的功能还挺强大的，才提出的时候，就可以用来对20×20像素的圆形，三角形以及正方形进行分类。那时候人工神经网络的研究可谓是风光无限，大量人员涌入，大量资本进入，就跟现在一样。<br>但感知机有一个巨大的缺陷。如果你仔细观察计算兴奋度的式子，就会发现它和最普通的线性模型几乎一样，只不过概念上使用了生物学的神经元模型来包装而已。实际上，感知机确实对非线性划分问题无能为力。<br>比如说异或计算问题：<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/NN/%E5%BC%82%E6%88%96.png" alt="图片4"><br>图4. 来源：《机器学习》——周志华<br>无论如何改变参数的值，都无法完成异或计算，这是因为无法在平面上找到一条直线把$(0,1),(1,0)$与$(0,0),(1,1)$分为两类。<br>所以说号称模拟生物神经元开发而来的人工MP神经元，连异或这种基本的非线性分类都做不到。这使得MP神经元的局限非常大，简直致命。而这个局限，被明斯基以一种充满敌意的方式呈现在了书中，直接导致神经网络的研究步入寒冬。</p>
<h3 id="3-多层前馈神经网络"><a href="#3-多层前馈神经网络" class="headerlink" title="3. 多层前馈神经网络"></a>3. 多层前馈神经网络</h3><p>说实话，现在往回看，研究停滞这件事的出现多少有点不可思议。<br>从生物学上来讲，绝大多数生物都拥有不止一个细胞，当单个MP神经元出现局限的时候，很自然的会想到在感知机的基础上加更多的神经元，看看是否会让感知机变得更加强大。<br>从逻辑计算的角度上来讲，已知感知机可以实现与、或、非门。那么用多个感知机构造一个异或门，讲道理不是什么难事。<br>而实际上只要在感知机的基础上再加一层神经元就可以进行异或的运算了。如下图：<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/NN/%E5%BC%82%E6%88%96%E6%84%9F%E7%9F%A5%E6%9C%BA.png" alt="图片5"><br>图5. 来源：《机器学习》——周志华<br>上面的这个网络结构，箭头上的1和-1是权重$w$，大家可以把四个点带进去试试，确实可以成功的进行异或运算。而这种在感知机上多加一层神经元组成的网络，就是我们的主角——多层前馈神经网络，简称神经网络。<br>功能上来看，这个神经网络已经足够强大了，不过这种结构下有两个问题是难以回答的：<br>这网络是什么意思？<br>参数如何训练？<br>因为输出只有0和1，那么参数的变化并不一定会导致网络整体输出的变化。比如说，将上图的阈值0.5换为0.1甚至0.000001，他还是能够进行异或运算。这种改变了参数，输出却没有变化的情况，会对我们理解网络造成困难，进而导致模型训练困难。因为参数改变了结果也不变，那么在结果不理想的情况下，我们该如何调整参数？变大？还是变小？还是怎么办？这是一个问题。<br>造成这个问题的原因是阶梯函数是一个不连续不可导的函数，用他作激活函数使得输出只有0，1两种，很僵硬。解决起来也简单，换一个激活函数就好。<br>这里不能用线性函数，因为用线性函数作为激活函数的神经元，再多个，再多层，连接起来，最终网络的整体输出$Y$，一定可以写成一个关于输入$X$的线性函数。也就是说全部使用线性函数作为激活函数的神经网络，无论多少层都只是一个结构更加复杂的线性模型，实际功能上和感知机没有任何区别。<br>那应该使用什么函数呢？首先线性是行不通了，那他起码是一个非线性函数。而阶梯函数这种的又会导致理解困难，所以他最好还是一个连续的函数。回顾过往的研究，我们恰好有一个这样的函数，他就是Sigmoid函数。该函数有非常多的优点，它非线性，连续，可导，导数好求，求出来的结果还不复杂。表达式如下：<br>$$S(x)=\frac {1}{1+e^{-x}}$$<br>现在我们把图5中所有神经元的激活函数都换成$Sigmoid$函数，它所呈现的结构就是多层前馈神经网络的基本结构了。下面给出多层前馈神经网络的具体解释：<br>多层指的是：在输入层及输出层中间还有拥有激活函数的激活层，隐含层至少一层，也可以有多层，也就是说一个多层前馈神经网络最少有三层，多则没有上限。示意图如下：<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/NN/%E5%A4%9A%E5%B1%82%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.jpg" alt="图片5"><br>图6. 来源：《机器学习》——周志华<br>上图中的 $\nu$代表隐层输入权重， $w$ 代表输出层输入权重，本文讨论的重点就是上图这种单隐含层神经元的网络，也叫三层神经网络。<br>前馈指的是：由于该网络每层神经元与下一层神经元全互连，同层神经元之间不互连，也不会跨层连接，所以信息只会按照信息只能按照输入—&gt;隐藏—&gt;输出的方向传递，得名前馈。<br>那么费劲半天，又是多加层神经元，又是寻找理想的激活函数，终于构造出这么个网络，有多牛逼？这些努力值得吗？<br>非常值，有理论证明，一个具有有限数目神经元的单隐层多层前馈神经网络可以以任意精度逼近任意复杂度的连续函数。换句话说，单隐层神经网络可以学习任意连续函数。如果我们把生活的问题全都理解为函数，中译英其实就是输入中文，输出英文的函数；P图就是输入原图，输出美图的函数；语音识别就是输入语音，输出文字的函数；那么单隐层网络，理论上，可以解决这些问题。有多牛逼？大概就是为所欲为的那种程度的牛逼吧。<br>理论很丰满，现实很骨干，这个理论上存在的参数该如何取得可是难为了人们好多年，直到BP算法的出现，训练的问题才算是找到了合理的解。</p>
<h3 id="4-误差逆传播算法（BP算法）"><a href="#4-误差逆传播算法（BP算法）" class="headerlink" title="4. 误差逆传播算法（BP算法）"></a>4. 误差逆传播算法（BP算法）</h3><h4 id="4-1-为什么训练神经网络是一件难事？"><a href="#4-1-为什么训练神经网络是一件难事？" class="headerlink" title="4.1 为什么训练神经网络是一件难事？"></a>4.1 为什么训练神经网络是一件难事？</h4><p>在介绍BP算法之前，我想先解释清楚为什么训练神经网络是一件难事。这对我们理解BP算法起到至关重要的作用。<br>和训练线性模型一样，训练神经网络本质上也是要找到一组参数 $(W,\theta)$ ，使得代价函数最小。通常来讲，都是用梯度下降法来解决这个问题。既然是梯度下降，那就求参数偏导数不就可以解决了吗，Sigmoid导数性质那么好，这又什么难度？<br>难度来自于神经网络的特殊结构。神经网络的全互联结构，使得神经网络有着数量庞大的参数总量，这无疑会降低网络训练速度。而神经网络的层级结构，使得它的输出y是一个复合了好几次的复合函数。如果我们直接求导，那么根据链式求导的规则，每一个参数的偏导数会写成一个看起来就很复杂的式子，中间不仅有乘法，有括号，还有求和符号 $\Sigma$ ，毫无疑问这样的公式算起来效率是很低的。<strong>所以问题不是别的，就是求各个参数的偏导太慢了。因此我们需要一个更加高效的方法来求偏导，而这个高效的方法就是BP算法。</strong></p>
<h4 id="4-2-如何理解BP算法？"><a href="#4-2-如何理解BP算法？" class="headerlink" title="4.2 如何理解BP算法？"></a>4.2 如何理解BP算法？</h4><p>BP算法：英文名：Error Back Propagation。译文：误差逆传播算法。由于其缩写是BP算法，所以也有称这个为反向传播算法的。<br>在解释BP算法为什么快之前，首先让我们回归问题本源：找到一组参数 $(W,\theta)$ 使得整体的输出值与实际值的误差最小。<br>在这句话中默认了一个条件：有一组理想化的参数 $(W,\theta)$ 可以使得误差最小。而现在之所以有这么大的误差，就是因为目前这组 $(W,\theta)$ 和理想的 $(W,\theta)$ 之间不一样。但是整体上的认知并没有什么意义，我们想知道每一个参数偏差了多少，便于我们调整。说起来这个过程，特别像比赛结束后的分锅大会，赛后采访教练团说的“这次比赛失利是教练组和全体队员的失利”，这种结论是毫无营养的，观众更希望分锅到户。<br>讨论如何分锅就必须要分析锅从何而来，整体分析是困难的，不妨从单独一个参数入手进行分析，比如说：在图6中，如果 $\nu_{1h}$ 的值与理想值不同，假设差异为 $\Delta\nu$ ，这里的误差会造成隐层第h个神经元的输出 $b_h$ 产生 $\frac{\partial b_h}{\partial \nu_{ih}}\Delta\nu$ 的误差。而 $b_h$ 又是所有输出层神经元的输入，所以会影响到所有输出层神经元的输出，最终造成总代价产生 $\frac{\partial CostFunction}{\partial \nu_{ih}}\Delta\nu$ 的误差。<br>对于每个参数来说都是如此，本身的误差会它连接的神经元开始，一层一层的向前传递，传到输出层，最终导致代价函数的值产生变化。既然锅是一层一层的传过来的，那么在分锅的时候，就自然而然的会想到反向一层一层的把锅分回去。而每一层的误差确实可以用上一层（上指的是更靠近输出端）的误差来表示。我们只要从输出层开始往回一层一层捋，捋一遍就可以把锅分明白。这就是反向传播的基本思想。<br>但是这和快速求导有什么关系？这是因为<strong>偏导数可以看成误差的度量。</strong><br>现在思考这样一个问题，已知一组 $(W,\theta)$ 的值，并且知道此时代价函数的值，我们可以人为的调整其中某个参数的值来降低代价，该怎样做？根据上面的式子  $\frac{\partial CostFunction}{\partial \nu_{ih}}\Delta\nu$ ，我们只需要将 $\Delta\nu$ 的符号与参数偏导的符号设置成相反的，就可以降低代价。<strong>在 $\Delta\nu$ 一定时，偏导数越大，代价变化越大，偏导数越小，代价变化越小。</strong>在这种情境下，我们可以把参数的偏导看作误差的度量。因为偏导越大，产生的误差就越大。<br>所以说偏导数的计算也可以按照分锅的思路，从代价函数-&gt;输出层-&gt;隐层，一层一层反向传播，每一层的偏导数都用上一层的偏导数来表示，以此通过一次反向传播就把所有参数的偏导数都求解出来。这就是BP算法快速求导的原理。<br>至于公式推导这里就不抄书了，有兴趣的可以看看《机器学习》中的推导。<br>实际上关于BP算法还可以从编程的角度上来解释，把求导理解为寻路，说不定理解起来更加轻松，具体的解释知乎中就有，大家可以自行搜索。<br>这里执意使用误差来解释，一是因为大部分学习资料没有覆盖到这种解释，而不解释的推导看起来和链式法则直接求导没有什么区别，这给BP算法的理解造成了不小的困扰；二是因为这种解释可以更深入的了解神经网络。比如说，都说梯度爆炸和消失会导致网络训练困难，但是这是为什么？而理解了误差，就可以解释了，因为梯度爆炸会导致该参数对代价函数产生极大的影响，而梯度消失则意味着该参数无法对代价函数产生影响。<br>偏导数计算出来后，训练就是梯度下降法，这里的梯度下降和线性回归中的没什么区别，如果你对此不太了解，可以参照这个连接：<a href="https://wasedamagina.github.io/archives/。" target="_blank" rel="external">https://wasedamagina.github.io/archives/。</a></p>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><ol>
<li>MP神经元是神经网络的基本单元。</li>
<li>虽然感知机无法处理非线性问题，但是在感知机的基础上再加一层神经元就可以了。这种多层的结构，就是神经网络的雏形。</li>
<li>多层前馈神经网络指的是有至少一个隐含层，且每层神经元与下一层神经元全互连，同层神经元之间不互连，也不会跨层连接的神经网络。神经网络一般指代这种网络。</li>
<li>BP算法是一种快速求导的方法。</li>
<li>误差是一层一层向前传递的，所以也可以反向逐层表示。</li>
<li>可以把偏导数理解为误差的度量，按照误差逆传播的思想来逐层计算偏导数。</li>
</ol>
<h3 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6. Reference"></a>6. Reference</h3><p>[1] 周志华. 机器学习 : = Machine learning[M]. 清华大学出版社, 2016.<br>[2] Michael A. Nielsen, “Neural Networks and Deep Learning”,Determination Press, 2015。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/23/Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/23/Logistic-Regression/" itemprop="url">逻辑回归(Logistic_Regression)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-23T00:07:28+09:00">
                2017-11-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习心得/" itemprop="url" rel="index">
                    <span itemprop="name">学习心得</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="逻辑回归的由来"><a href="#逻辑回归的由来" class="headerlink" title="逻辑回归的由来"></a>逻辑回归的由来</h3><p>线性回归能够做到具体值的预测，但实际生活中大家也有很多分类需求，比如说西瓜甜不甜，IPhoneX值不值得买，网线那头使劲卖萌的网友是萌妹还是大屌萌妹等。这时候线性回归就显得不那么灵活了。那么我们能不能通过稍微改进一下线性回归，使他可以进行分类呢？要做到这件事，首先需要一个函数把线性方程的值域映射到(0,1)。这样做有一个好处，就是函数的结果可以用来表示分类到1的概率，比如说计算结果是0.6，那就是说分类到1的概率为60%。那么这个函数最好可以在0.5附近变化最快，这样我们更不容易得到0.5附近的结果，使得看上去更可靠。最终人们找到了一个符合上述要求的函数——逻辑斯蒂函数(Logistic Function)，这大概就是逻辑回归的由来吧。</p>
<h3 id="逻辑斯蒂函数-Logistic-Function"><a href="#逻辑斯蒂函数-Logistic-Function" class="headerlink" title="逻辑斯蒂函数(Logistic Function)"></a>逻辑斯蒂函数(Logistic Function)</h3><p>该函数也叫Sigmoid函数，在神经网络中会再次提到。他的表达式如下：<br>$$<br>S(x)=\frac {1}{1+e^{-x}}<br>$$<br>他的图像如下：</p>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/LR/600px-Logistic-curve.svg.png" alt="600px-Logistic-curve.svg"></p>
<p>可以看到满足上述性质。除此之外他还有一点好处，求导很容易：<br>$$<br>S^{‘}(x)=\frac {e^{-x}}{(1+e^{-x})^2}=\frac {1}{1+e^{-x}}(1-\frac {1}{1+e^{-x}})=S(x)(1-S(x))<br>$$</p>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>和线性回归一样，逻辑回归也是想找到一组最优的参数值，我们先写出逻辑回归的 $h_\theta(x)$ ：<br>$$<br>h_\theta(x)=S(\theta^Tx)=\frac {1}{1+e^{-\theta^Tx}}<br>$$<br>这个方程通过差的平方和来构造代价函数来求最小值是不太好用的，因为那是一个非凸函数，存在多个极值点，无论是使用梯度下降还是最小二乘法都有可能陷入局部最有解。因此我们使用极大似然估计来寻找那一组最优的参数值。那么首先我们需要表示出在参数$\theta$和输入x确定的条件下，y出现的概率。根据我们上文说的函数的结果可以用来表示分类到1的概率，我们可以写出下面两个式子：<br>$$<br>P(y=1|x;\theta)=h_\theta(x) \\<br>P(y=0|x;\theta)=1-h_\theta(x)<br>$$<br>我们可以用一个小技巧将其合并在一起：<br>$$<br>p(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}<br>$$<br>看起来很复杂，其实很简单。$y=1$时，后一半没了，得到式子1，$y=0$时，前一半没了得到式子2。</p>
<p>根据极大似然估计的思想，我们想要在参数$\theta$和输入x确定的条件下，所有样本的真实值y出现的概率越大越好，那么我们需要求一个联合概率，因为他们相互独立所以我们可以把所有y出现的概率连乘，得到likelihood函数：<br>$$<br>L(\theta)= \prod_{i=1}^{m}(h_\theta(x)^y(1-h_\theta(x))^{1-y}<br>$$<br>我们需要求上式的最大值，连乘的形式很难求，用$\log$函数转化为累加就比较好解决了：<br>$$<br>\begin{align}<br>l(\theta)&amp;=\log L(\theta) \\<br>&amp;=\sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log (1-h_\theta(x^{(i)}))<br>\end{align}<br>$$<br>上式是一个凸函数，极值点就是最值点，而且可导，因此使用极大似然估计是优于差的平方和构造代价函数的。</p>
<p>我们求其对每一个$\theta$的偏导：</p>
<p>$$<br>\begin{align}<br>\frac {\partial}{\partial\theta_j}l(\theta) &amp;=\sum_{i=1}^{m}(y^{(i)}\frac 1{ h_\theta(x^{(i)})}-(1-y^{(i)})\frac 1{ (1-h_\theta(x^{(i)}))})\frac {\partial}{\partial\theta_j}h_\theta(x^{(i)}) \\<br>&amp;= \sum_{i=1}^{m}(y^{(i)}\frac 1{ h_\theta(x^{(i)})}-(1-y^{(i)})\frac 1{ (1-h_\theta(x^{(i)}))})h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))\frac {\partial}{\partial\theta_j}\theta^Tx^{(i)} \\<br>&amp;= \sum_{i=1}^{m}(y^{(i)}(1-h_\theta(x^{(i)}))-(1-y^{(i)})h_\theta(x^{(i)}))x_j^{(i)}\\<br>&amp;=  \sum_{i=1}^{m}(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}<br>\end{align}<br>$$</p>
<p>上面这个式子只是用了几次求导中的链式求导法则而已，属于高中数学，注意下 </p>
<p>$$<br>h_\theta(x)=S(\theta^Tx)=\frac {1}{1+e^{-\theta^Tx}}<br>$$ </p>
<p>之后一步一步看，很容易看明白。</p>
<p>因为这里我们是求最大值，所以要用梯度上升而不是下降，每个$\theta$的更新法则如下：<br>$$<br>\theta_j:=\theta_j+\alpha \frac {\partial}{\partial\theta_j}l(\theta)=\theta_j+\alpha\sum_{i=1}^{m}(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}_j<br>$$<br>重复上式，直到收敛，即可求得最大值，而使其取得最大值的$\theta$就是我们找的那一组最优的参数。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>至此，逻辑回归的基本思想已经解释完毕。其实就是线性回归套上了一个逻辑斯蒂函数，把值域映射到了(0,1)之间而已。大部分的知识比如说极大似然估计，梯度上升法，都在线性回归中介绍过了。如果你看不懂这两个，可以去线性回归2和线性回归3中找答案。</p>
<p>最后，本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>斯坦福机器学习公开课讲义note1</p>
<p>知乎-逻辑回归损失函数为什么使用最大似然估计而不用最小二乘法 zzanswer的回答</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/21/Linear-Regression3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/21/Linear-Regression3/" itemprop="url">线性回归(3)——求最小值的两种方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-21T16:43:39+09:00">
                2017-11-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习心得/" itemprop="url" rel="index">
                    <span itemprop="name">学习心得</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="求-J-theta-最小值"><a href="#求-J-theta-最小值" class="headerlink" title="求$J(\theta)$最小值"></a>求$J(\theta)$最小值</h3><p>根据我们的假设，我们希望$J(\theta)$越小越好。求解$J(\theta)$的最小值，有两种方法，一种为梯度下降法，一种为最小二乘法。别看这两个名字都很不明觉厉，其实核心思想高中我们就学过——求导。</p>
<h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>核心思想：因为x处的导数值为原函数在点(x,f(x))处切线的斜率，通过求导可以得到函数下降最快的方向，之后不断迭代，即可不断逼近函数的最小值。</p>
<p>对于上面这个例子来说，因为有多个$\theta$，我们需要对多个$\theta$求偏导，并按照下面的规则更新$\theta$值：<br>$$<br>\theta_j:=\theta_j-\alpha \frac {\partial}{\partial\theta_j}J(\theta)<br>$$<br>$\alpha$是学习率也叫步长，如果取值太大，容易跳过最小值，在最小值附近振荡。如果取值太小，会使得下降次数增多，导致收敛过慢。所以说步长这个别称还是挺有意思的，步子太小走得又慢，步子太大容易扯着蛋。</p>
<p>求偏导：<br>$$<br>\begin{align}<br>\frac {\partial}{\partial\theta_j}J(\theta) &amp;=\frac {\partial}{\partial\theta_j}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 \\<br>&amp;= \sum_{i=1}^m2(h_\theta(x^{(i)})-y^{(i)})\frac {\partial}{\partial\theta_j}(h_\theta(x^{(i)})-y^{(i)}) \\<br>&amp;= \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) x_j<br>\end{align}<br>$$<br>有了偏导值，我们可以简单的写出该问题下梯度下降法的步骤：</p>
<p>不断重复下面这个式子，用来更新$\theta$值，直到收敛。<br>$$<br>\theta_j=\theta_j-\alpha \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) x_j<br>$$</p>
<h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><p>核心思想：导数为0的地方为极值点，最小值就在那里。<br>这里你可能会有疑问，导数为零的地方只是极值点，有可能是极大值，极小值，不一定是最小值。<br>首先$J(\theta)$的形式决定了他只有最小值，没有最大值。其次$J(\theta)$是凸函数，所以他的局部极小值就是全局最小值<br>。至于$J(\theta)$为什么是凸函数。。。我不知道，反正他就是。Andrew Ng说的。<br>因为我们计算的$\theta$,$x$,$y$实际上都是矩阵，所以需要用到线性代数中求梯度的知识来计算，这部分我不打算细说，具体的推导步骤如下。</p>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/LR/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20171118182715.png" alt="微信图片_20171118182715"></p>
<p>然后令导数得零，即可获得$\theta$的值。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>线性回归因为内容比较多，分了三篇文章来叙述，现在已经基本说明白了，我们来总结一下：</p>
<ol>
<li>线性回归由高尔顿提出，其思想是几乎所有的科学观察都着了魔似的向平均值回归。</li>
<li>线性回归的根本任务是确定一组参数，使得预测值和真实值之间的差异最小。</li>
<li>可以通过极大似然估计来论证为什么要使用差的平方和来描述预测值和真实值之间的差异。</li>
<li>极大似然估计认为对于给定的参数来说，最好的那一组应该是会让目前现存的所有样本出现概率最大的那组。</li>
<li>我们可以使用梯度下降和最小二乘法来求解损失函数的最小值。</li>
<li>梯度下降法的思想是过求导可以得到函数下降最快的方向，之后不断迭代，即可不断逼近函数的最小值。</li>
<li>最小二乘法的思想是导数为0的地方为极值点，最小值就在那里。</li>
</ol>
<p>最后，本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>《线性回归：机器学习史上最大命名错案》——机器学习入门微信公众号</p>
<p>斯坦福机器学习公开课讲义note1</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/21/Linear-Regression2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/21/Linear-Regression2/" itemprop="url">线性回归(2)——代价函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-21T15:27:50+09:00">
                2017-11-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习心得/" itemprop="url" rel="index">
                    <span itemprop="name">学习心得</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="代价函数-cost-function"><a href="#代价函数-cost-function" class="headerlink" title="代价函数(cost function)"></a>代价函数(cost function)</h3><p>上一篇文章我们说到，线性回归需要找到一条直线来拟合所有样本点，那么我们如何来判断他是否拟合了呢？我们需要将直线和样本点的偏差量化，用一个函数来表达，之后求其最小值，就可以得到我们想要的参数了。而这个函数就叫做代价函数(cost function)。</p>
<p>为了方便后面的分析，下面我需要使用斯坦福机器学习公开课中的例子来进行解释：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Living area (feet2)</th>
<th style="text-align:center">bedrooms</th>
<th style="text-align:center">Price (1000$s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2104</td>
<td style="text-align:center">3</td>
<td style="text-align:center">400</td>
</tr>
<tr>
<td style="text-align:center">1600</td>
<td style="text-align:center">3</td>
<td style="text-align:center">330</td>
</tr>
<tr>
<td style="text-align:center">2400</td>
<td style="text-align:center">3</td>
<td style="text-align:center">369</td>
</tr>
<tr>
<td style="text-align:center">1416</td>
<td style="text-align:center">2</td>
<td style="text-align:center">232</td>
</tr>
<tr>
<td style="text-align:center">3000</td>
<td style="text-align:center">4</td>
<td style="text-align:center">540</td>
</tr>
<tr>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
</tr>
<tr>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
</tr>
</tbody>
</table>
<p>上图是一组房屋的数据，包含了房屋大小，卧室数量，以及售价。问题是，想要根据房屋大小以及卧室的数量来预测其售价，那么首先我们可以用一个线性方程来定义售价：</p>
<p>$$<br>h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2<br>$$</p>
<p>h是hypotheses的简写。如果我们设$x_0=1$，那么上式可以简写为：</p>
<p>$$<br>h(x)=\sum_{i=0}^{n}\theta_ix_i=\theta^Tx<br>$$</p>
<p>其中$n=2$，表示的是特征数量，如果加入了其他和房价相关的特征，比如说离学校的距离，离地铁的距离等其他特征，n的数值会变大，式子形式还是一样。$\theta^T$和$x$为矩阵。而整个线性回归问题，就是为了确定一组参数$\theta^T$使得预测值和真实值的差异最小。</p>
<p>我们使用差的平方和来计算预测值和真实值之间的差异，所有差异之和就可以用下式表示：</p>
<p>$$<br>J(\theta)=\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2<br>$$</p>
<p>其中$h_\theta(x^{(i)})$代表第$i$个预测值，$y^{(i)}$代表第$i$个真实值。差异值最小，也就是$J(\theta)$最小。</p>
<p>这里大家可能会有疑问，为什么要用欧氏距离来计算差异？明明表示的是价钱差，直接相减取绝对值，不用平方不是可以更直观的表示吗？这个可以通过概率学来解释。</p>
<h4 id="J-theta-来历的解释"><a href="#J-theta-来历的解释" class="headerlink" title="$J(\theta)$来历的解释"></a>$J(\theta)$来历的解释</h4><p>$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2$中的 $\theta_0$ 是误差，误差表示与特征无关的常量影响，或者是随机噪音。 $h_\theta(x)$ 可以写成下面的形式：</p>
<p>$$<br>y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}<br>$$</p>
<p>假设所有的$\epsilon^{(i)}$是独立同分布的，并且满服从均值为0，方差为$\sigma^2$的高斯分布。我们可以写出$\epsilon^{(i)}$的概率密度函数：<br>$$<br>p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}).<br>$$<br>我知道这个式子看起来一点也不简单，但是他实际上只是一个高斯分布的概率密度函数，如果你懂概率论那你一定不陌生，如果你不懂，没关系，暂时不去纠结为什么是这样，把他当作一个公式记住，并知道他描述的是概率就好。</p>
<p>因为$\epsilon^{(i)}=y^{(i)}-\theta^Tx^{(i)}$，把他带入，我们可以写出下面的表达式。<br>$$<br>p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}).<br>$$<br>上式也是一个概率密度函数，描述的是对于给定的$\theta$和$x^{(i)}$，$y^{(i)}$出现的概率。</p>
<h5 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h5><p>这个东西有什么用？我们回到问题的出发点，想要找到一组参数，使得方程可以拟合所有的样本点。那么对于给定的参数来说，最好的那一组参数应该是会让目前现存的所有样本出现概率最大的那一组，这就是极大似然估计的中心想法。我们将所有样本在该参数下的出现的概率连乘，就可以得到likelihood函数，写为下式：</p>
<p>$$<br>L(\theta)=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}).<br>$$</p>
<p>目的是求出该函数的最大值，但是连乘问题不好解决，我们通过对数函数将其转化为连加来解决：</p>
<p>$$<br>\begin{align}<br>l(\theta)&amp;=\log L(\theta) \\<br>&amp;= \log \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\<br>&amp;=\sum_{i=1}^{m}\log \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\<br>&amp;= m\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2<br>\end{align}<br>$$</p>
<p>这个式子看起来很复杂，其实只用了对数的运算法则而已，第二步用的是$\log mn=\log m + \log n$，第三步用的是$\log m^n=n\log m$。相信大家稍微看一下就能明白是怎么回事。</p>
<p>因为该式第一项是个常数，所以只要使得第二项最小，就可以使得整体最大。而第二项……那不就和$J(\theta)$一样吗？这也就解释了为什么代价函数是真实值与预测值的差的平方和。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章介绍了线性回归代价函数的由来，和最大似然估计法。<br>由于本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>《线性回归：机器学习史上最大命名错案》——机器学习入门微信公众号</p>
<p>斯坦福机器学习公开课讲义note1</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/13/Linear_Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/13/Linear_Regression/" itemprop="url">线性回归(1)——起源</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-13T18:10:36+09:00">
                2017-11-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习心得/" itemprop="url" rel="index">
                    <span itemprop="name">学习心得</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>几乎所有的科学观察都着了魔似的向平均值回归——《女士品茶》</p>
</blockquote>
<h3 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h3><p>线性回归这个概念是由达尔文的表弟高尔顿在研究父代与子代身高关系的时候提出的，我第一次看到这四个字的时候，心中暗骂，这起的什么破名，一点都不直观。什么叫线性？什么叫回归？你在进行什么骚操作啊。然而这两个概念其实准确表达了该算法的核心思想，只要解释明白了这两个概念，我们就搞明白了线性回归。</p>
<h4 id="线性"><a href="#线性" class="headerlink" title="线性"></a>线性</h4><p>高尔顿搜集了1078对父亲及其儿子的身高数据，用于研究其两者的关系，他画出了该组数据的散点图，发现这些样本点看起来分布在某条直线的周围，因此他使用一条直线来拟合这些样本点。</p>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/LR/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20171116215924.jpg" alt="微信图片_20171116215924"></p>
<p>这也就是线性最初的意思：所有的样本点可以近似的用一条直线来表示。</p>
<h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><p>高尔顿在画出直线后，对这些数据进行了深入的分析，最终发现了一个很有趣的现象。他发现当父亲高于平均身高时，他们的儿子更可能比他矮；而当父亲矮于平均身高时，他们的儿子身高更可能比他高。他称这种现象为回归现象。</p>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/LR/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20171116233000.jpg" alt="微信图片_20171116233000"></p>
<p>参考上图：</p>
<p>只看x轴，我们可以简单的估计一下，父亲们的平均身高大概在68左右。然后看上图中标黄的部分，身高为64左右的父亲们，他们的儿子身高均值为67.1，普遍比父亲要高，向着均值68的方向贴近。再看绿色部分，身高为72左右的父亲们，儿子们的身高均值为70.9，普遍比父亲矮，向着均值68的方向贴近。</p>
<p>高尔顿将这种子代身高向着父辈的平均身高靠拢的现象称为回归效应。他还说，如果不向着均值的方向回归，高个子的后代更高，矮的更矮的话，用不了几代，我们人类就可以分裂成两个种族——巨人族和矮人族了。不仅身高如此，</p>
<blockquote>
<p>几乎所有的科学观察都着了魔似的向平均值回归，这个世界观可以帮我们理解一些荒唐的问题：人类的身高不可能一直越来越高。人类的举重能力不可能越来越大。基因突变不会一直累积，所以变种人不会出现。行尸走肉里那种全球瘟疫不会发生。帅气的爷爷和爸爸，不一定有帅气的孙子。富不过三代。大自然（或者人类社会）以其神秘的力量和节奏调节着地球万物的秩序。——机器学习入门公众号</p>
</blockquote>
<h4 id="和机器学习有什么关系？"><a href="#和机器学习有什么关系？" class="headerlink" title="和机器学习有什么关系？"></a>和机器学习有什么关系？</h4><p>最后高尔顿一通计算得到了拟合直线的表达式：<br>$$<br>y=33.73+0.516x<br>$$<br>其中x代表父辈的身高。</p>
<p>戏说：高尔顿把（33.73，0.516）这一对数字卖给算命先生A，告诉A，以后你也别算命了，支个摊子，叫算高先生，帮别人算孩子以后能长多高吧。让顾客告诉你他们的身高，然后代到上面的式子里算一下，结果就是他们孩子的身高。别说，这样算出来的结果虽然不百分百准确，但是八九不离十。因此A先生的算高摊子备受好评。</p>
<p>上面这个瞎编的故事，就是线性回归与机器学习的关系。线性回归是一种机器学习的方法，可以根据训练集（高尔顿收集的1078组身高数据）训练得到一组参数（33.73，0.516），之后利用该参数来预测结果（A先生做的事）。所以线性回归的根本问题就是通过已有训练集的数据，得到线性模型的一组参数。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们总说机器学习机器学习，机器到底是从什么地方，学到了什么呢？线性回归这个特别基本的方法就很好的回答了这个问题——从一堆样本点里面学到了一组参数。我想这种直观的特点可以解释为什么几乎所有机器学习的书和教程上手的第一个算法是线性回归。</p>
<p>最后，本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>《线性回归：机器学习史上最大命名错案》——机器学习入门微信公众号</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/12/Knn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/12/Knn/" itemprop="url">K-Nearest Neighbor algorithm(KNN)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-12T21:41:11+09:00">
                2017-11-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习心得/" itemprop="url" rel="index">
                    <span itemprop="name">学习心得</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>近朱者赤，近墨者黑——《太子少傅箴》</p>
</blockquote>
<h3 id="什么是KNN算法"><a href="#什么是KNN算法" class="headerlink" title="什么是KNN算法"></a>什么是KNN算法</h3><p>KNN算法，全称K-Nearest Neighbor algorithm，直译就是K个最近的邻居算法，一般称为K最近邻法。这是一种属于监督学习的分类算法。</p>
<p>其中心思想用八个字就能概括：近朱者赤，近墨者黑。和原意有稍许不同，这里的近朱者赤近墨者黑是指；虽然我不知道你是红是黑，但我只要看你离红的近还是黑的近就好了，红近则为红，黑近则为黑。</p>
<p>生活中我们也常常用这样的方法认识他人。比如：因为小亮爸爸是混社会的，所以他一定也不是什么好人，不许和他玩；小明天天去台球厅和混混打台球，所以他一定也是不良少年；李磊的朋友都是韩梅梅和李华这种英语课本中的人物，所以他的英文肯定也非常好。这种根据A周围人的属性，来推断A属性的思想，就是KNN算法的核心思想。用更加正式一点的表达就是：</p>
<blockquote>
<p>给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例多数属于哪类，就认为输入实例分类属于哪类。</p>
</blockquote>
<h3 id="KNN算法的关键"><a href="#KNN算法的关键" class="headerlink" title="KNN算法的关键"></a>KNN算法的关键</h3><p>KNN算法的思想非常容易理解，但若我们现在就开始上手编程实现KNN，就会遇到下面两个问题：</p>
<ol>
<li>什么叫离得近？怎么判断新来的实例和训练数据集中实例的邻近程度？</li>
<li>所谓的K个实例，到底是几个实例？为什么是个变量而不是最近的一个？</li>
</ol>
<p>这两个问题同样也是KNN算法的核心问题，会极大的影响KNN算法的性能。</p>
<h4 id="远近？距离！"><a href="#远近？距离！" class="headerlink" title="远近？距离！"></a>远近？距离！</h4><p>描述远近？这不就是距离干的事吗？而数学上有多种距离定义，其中最常用的是欧氏距离。你可能没听说过这个名字，但是你一定用过下面这个公式：<br>$$<br>|AB|=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}<br>$$<br>高中时代这个公式叫两点间距离公式，其实就是欧氏距离公式在二维空间的形式（维度的概念在线性回归中再细说，这里就简单的理解为下标的最大值就好）。对于两个n维向量，$x=(x_1,x_2,…,x_n)$,$y=(y_1,y_2,…,y_n)$他们之间的欧式距离为：<br>$$<br>d(x,y)=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+…+(x_n-y_n)^2}<br>$$<br>还有很多种其他距离，比如曼哈顿距离、切比雪夫距离、汉明距离等，这里就不一一介绍了，只是对于距离计算的方式不同而已，有兴趣的可以自己上网查一下。</p>
<h4 id="为什么取K个实例？K值如何确定？"><a href="#为什么取K个实例？K值如何确定？" class="headerlink" title="为什么取K个实例？K值如何确定？"></a>为什么取K个实例？K值如何确定？</h4><p>因为KNN算法的分类结果随K取值的不同会发生变化，下图简单的说明这一问题：</p>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/jpg/KNN/%E5%9B%BE%E7%89%871.png" alt="图片1"></p>
<p>红色和蓝色为已知训练集中的两个不同分类，现在来了一个新的输入，想要判断绿圆是属于红三角还是蓝方块。在K=3时，最近的3个实例中，有两个红三角，一个蓝方块，红三角比较多，所以新输入应该属于红三角。而K=5时，最近的5个实例中，有三个蓝方块，两个红三角，蓝方块比较多，所以新输入应该属于蓝方块。不同的K值使得KNN的分类结果不同了，这种不同最后会使得KNN的分类准确率也不同，所以找到一个最合适的K值是很关键的。</p>
<p>实际操作中，K值的确定往往使用交叉验证的方法，说白了就是试出来的。将训练集分20%的数据划分出去，作为测试集，取不同的K值记录各K值下的分类正确率，取正确率高的作为最终K值。</p>
<h3 id="KNN算法的步骤"><a href="#KNN算法的步骤" class="headerlink" title="KNN算法的步骤"></a>KNN算法的步骤</h3><p>有了上述背景，我们可以简单的写出KNN算法步骤：</p>
<ol>
<li>计算新入实例和各个训练样本的距离。</li>
<li>找到距离最小的K个实例。</li>
<li>统计这K个实例每个类别出现的次数。</li>
<li>将出现频率最高的类别记为新入实例的类别。</li>
</ol>
<h3 id="KNN算法的缺点"><a href="#KNN算法的缺点" class="headerlink" title="KNN算法的缺点"></a>KNN算法的缺点</h3><p>KNN算法的优点有很多，其中最明显的一点是，这个算法逻辑简单，易于理解。他的缺点也不少，主要是有下面几个：</p>
<ol>
<li>在样本数量不均衡时，KNN会偏向于数目更多的类别。举个例子，小亮的爸爸是黑社会，他有58个同班同学都是根正苗红的社会主义好青年，按照KNN算法小亮大概率被归为模范少年，但是这并不符合事实。</li>
<li>KNN算法需要进行繁重的距离计算：每来一个新的样本都要和所有样本进行一次距离计算，这在训练集很大的时候，会严重影响计算速度。</li>
<li>KNN算法需要存储所有训练样本，利用起来不方便。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>写到这里，KNN的核心思想及算法步骤已经全部讲解完毕。这是一个非常简单，理解起来很容易的算法。但是这并不是KNN的所有内容，为了解决KNN的各种缺陷，市面上有一系列的KNN改进算法，比如：通过引入权值来把距离考虑在内的；将训练集分组减小计算量的；使用KD树减少距离计算次数的。这些都属于KNN进阶版本的算法了，我将另开一篇文章对这些内容进行通俗易懂的解释。</p>
<p>最后，本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>CAWI2016-行为识别课程2-机器学习-K最近邻分类PPT</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/11/decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Magina">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/Pic.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Magina的个人小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/11/decision-tree/" itemprop="url">决策树-基本原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-11T23:33:28+09:00">
                2017-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/学习心得/" itemprop="url" rel="index">
                    <span itemprop="name">学习心得</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>一个事物内部会存在随机性，也就是不确定性，而从外部消除这个不确定性唯一的办法是引入信息。——吴军《数学之美》</p>
</blockquote>
<h3 id="什么是决策树"><a href="#什么是决策树" class="headerlink" title="什么是决策树"></a>什么是决策树</h3><p>生活中人们其实常常在潜意识中用到该模型，只不过大家没意识到而已。</p>
<p>比如，新学期伊始，AB两人在宿舍里开始了闲谈：</p>
<p>A：咱们这学期选什么课啊，听学长说吉江老师的课挺好的，要不咱们一起上？</p>
<p>B：好啊，吉江课好过嘛</p>
<p>A：好过的，据说给分还挺高的。</p>
<p>B：那有考试吗？</p>
<p>A：没有考试，只用交5个实验报告就好。</p>
<p>B：作业呢？</p>
<p>A：不多，只用交实验报告，而且还是按组交的，组内要是有大神，嘿嘿嘿。</p>
<p>B：那是不是平时分特别多啊，压力会不会很大。</p>
<p>A：不会不会，吉江一天到晚往国内跑拉项目，就上六次课，压力约等于零。</p>
<p>B：那好啊，咱一起上吧。</p>
<p>这里B同学的决策逻辑就是一个典型的决策树，我们可以用下图表示。<br><img src="http://ozaeyj71y.bkt.clouddn.com/image/decision_tree/tree.png" alt="tree"><br>可以看到，决策树其实就是一个用来做决策的树形结构。</p>
<p>正式的来定义一下决策树其实就是：</p>
<ol>
<li>是一个树形结构（二叉树或者多叉树）</li>
<li>非叶结点（长方形）代表一个特征</li>
<li>分支（箭头）代表输出</li>
<li>叶节点（圆形）代表决策结果</li>
</ol>
<h3 id="决策树的优点"><a href="#决策树的优点" class="headerlink" title="决策树的优点"></a>决策树的优点</h3><ol>
<li>非常简单，简单到你怀疑这到底是不是机器学习算法。</li>
<li>非常通俗易懂，看到树就明白是怎么回事了。</li>
</ol>
<h3 id="如何造树"><a href="#如何造树" class="headerlink" title="如何造树"></a>如何造树</h3><p>我们通过分析上图，能够提出两个问题：为什么B同学认为容不容易过，有没有考试，作业多不多，这三个要素是选择一门课的关键？以及为什么B同学为什么按照考试，作业，压力的顺序来进行提问。</p>
<p>正式点说就是：选取什么特征作为结点，以及用什么样的顺序来进行判断。这两点会成为构造决策树的关键。</p>
<h3 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h3><p>首先，我想明确一点：选取什么特征和给特征排序其实是一个问题。这个问题就是，什么特征，最为关键重要，最能左右我的决策。排序越靠后的特征，代表越不关键，越不能左右我的决定。既然如此，我就也没有必要将其加在决策树中了。</p>
<p>所以说，归根结底，我们只用解决一个问题：哪几个特征最能左右我的决策？这看上去是一个公说公有理婆说婆有理的问题，但是，恰好，我们有一个用来量化表示左右决策程度的量——信息熵增益。</p>
<p>通过简单的比较信息熵增益的大小，就可以判断出到底哪个特征更加重要。不过在介绍它之前，我需要先说明信息熵以及条件熵到底是什么。</p>
<h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>熵这个概念并不是我们第一次接触了，早在高中时代我们就接触过热力学的熵，如果我没有记错的话，熵是一个用来描述物质/系统混乱程度的物理量，熵越高，越混乱。类似的，信息熵是一个描述随机变量的不确定性的物理量，信息熵越大，不确定性越高，信息量越少。不确定性和混乱度，这两组概念是多么像啊，难怪命名为信息熵。</p>
<p>设X是一个有限取值的离散随机变量，其概率分布为：<br>$$<br>P(X=x_i)=P_i,i=1,2,3…n<br>$$</p>
<p>则随机变量X的信息熵定义如下：</p>
<p>$$<br>H(X)=-\sum_{i=1}^{n}p_i \times log(p_i)<br>$$</p>
<p>这里我不想解释信息熵为什么定义成这个样子，这会牵扯很多和机器学习不相关的领域。我只想说明一下为什么这个式子可以用来表述不确定性，以及对于决策树来说不确定性为什么这么重要。</p>
<p>对于一个二元分类来说，即n=2的情况下，信息熵的定义式可以写成如下形式：</p>
<p>$$<br>H(X)=-p \times log(p)-(1-p) \times log(1-p)<br>$$</p>
<p>我们可以将H(X)理解为一个关于概率p的函数，其函数图像如下（摘自百度百科）：</p>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/decision_tree/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%9B%BE%E5%83%8F.jpg" alt="H(x)"></p>
<p>上文说过，信息熵H代表的是不确定性，H越大不确定性越大，信息量越小。若我们将上图的P理解为明天是晴天的概率，那么1-P代表的就是是明天不是晴天的概率。P=0就意味着明天百分百不是晴天，不确定性为0，所以P=0时H=0。同样的P=1时，意味着明天百分百是晴天，不确定性也为0，所以P=1时H=0。在P=0.5时，意味着我们对于明天是不是晴天毫无概念，不知道明天会是什么天，也不能像P=0.6时，得出明天很有可能是晴天的结论，我们仅仅知道我们一无所知。此时不确定性最大，信息量最小，所以H也在P=0.5时取到了最大值。</p>
<p>由此可见，信息熵的变化就代表了不确定性的变化。而决策树的构造过程其实就一个不断引入信息来降低不确定性的过程，每引入一个特征都会使得信息熵产生一定程度的减小，而熵减小的程度就代表了特征的重要性，这个熵减小的程度就是信息熵增益。</p>
<h4 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h4><p>为了计算信息熵增益，首先要计算条件熵：</p>
<p>条件熵H(Y|X)表示在已知随机变量X的条件下，随机变量Y的不确定性。<br>$$<br>H(X|Y)=-\sum_{x\in X,y\in Y}P(x|y) \times log(x|y)<br>$$<br>换种说法就是，引入特征X后，随机变量Y的不确定性。</p>
<h4 id="信息熵增益"><a href="#信息熵增益" class="headerlink" title="信息熵增益"></a>信息熵增益</h4><p>可以看到，我们现在可以计算随机变量Y原本的不确定性，还可以计算引入特征X后的不确定性，那么我们简单的将两式子相减就可以得到引入特征X后，不确定性减少的程度，也就是信息熵增益。其表达式如下：<br>$$<br>g(Y,X)=H(Y)-H(Y|X)<br>$$</p>
<h3 id="构建决策树"><a href="#构建决策树" class="headerlink" title="构建决策树"></a>构建决策树</h3><p>有了熵，条件熵，和信息熵增益的概念，构建决策树就是一件非常简单的事情了。首先计算原始熵，然后计算引入各个特征之后的条件熵，分别算出每个特征的信息熵增益，将增益最大的特征放在第一个判断结点。重复上述步骤，直到没有特征剩余，决策树就构建完成了，非常简单。</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>现在各大社交网络上充斥着各种僵尸粉，我们能否通过构造决策树来判断一个账号是否真实存在呢？通过人工浏览，我们获取了十个账号的信息，包括日志密度，好友密度，是否使用真实头像并对其真实性进行了人工标注，信息如下：</p>
<p><img src="http://ozaeyj71y.bkt.clouddn.com/image/decision_tree/H.jpg%E4%BE%8B%E9%A2%98.png" alt="例题"></p>
<p>其中s，m和l分别代表小，中，大。</p>
<p>设L、F、H和R表示日志密度、好友密度、是否使用真实头像和账号是否真实。下面计算各属性的信息增益。</p>
<p>首先计算原始熵：</p>
<p>$$<br>H(R)= -0.7 \times log(0.7)-0.3 \times log(0.3)=0.7\times0.51+0.3\times1.74=0.879<br>$$</p>
<p>然后计算引入日志密度后的条件熵：<br>$$<br>H(L|R)=0.3\times(-\frac{1}{3}log\frac{1}{3}-\frac{2}{3}log\frac{2}{3})+0.4\times(-\frac{1}{4}log\frac{1}{4}-\frac{3}{4}log\frac{3}{4})+<br>0.3\times(-\frac{0}{3}log\frac{0}{3}-\frac{3}{3}log\frac{3}{3})=0.602<br>$$<br>上式分为三部分，账号日志密度为s的概率为3/10=0.3，在此条件下，账号为真的概率为1/3，假的概率为2/3，可以得到上式第一部分。日志密度为l和m时同理。</p>
<p>计算日志密度的熵增益：<br>$$<br>gain(L,R)=H(R)-H(L|R)=0.879-0.602=0.277<br>$$<br>计算另一个特征好友密度的条件熵：<br>$$<br>\begin{align}<br>H(F|R)=0.4\times(-\frac{1}{4}log\frac{1}{4}-\frac{3}{4}log\frac{3}{4})+ \ 0.4\times(-\frac{0}{4}log\frac{0}{4}-\frac{4}{4}log\frac{4}{4})+ \<br>0.2\times(-\frac{0}{2}log\frac{0}{2}-\frac{2}{2}log\frac{2}{2})=0.325<br>\end{align}<br>$$<br>计算其熵增益：<br>$$<br>gain(F,R)=H(R)-H(F|R)=0.879-0.325=0.554<br>$$<br>计算最后一个特征真实头像的条件熵：<br>$$<br>H(H|R)=0.5\times(-\frac{2}{5}log\frac{2}{5}-\frac{3}{5}log\frac{3}{5})+0.5\times(-\frac{1}{5}log\frac{1}{5}-\frac{4}{5}log\frac{4}{5})=0.846<br>$$<br>计算其熵增益：<br>$$<br>gain(H,R)=H(R)-H(H|R)=0.879-0.846=0.023<br>$$<br>可以看到，三个熵增益中，好友密度的熵增益最大，因此将其放在第一个判断结点。之后重复上面的三个步骤，确定后两个特征的顺序，即可构造出整棵决策树。</p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>写到这里，决策树的基本思想就已经全部介绍完毕了。但是这并不是决策树的所有内容，按照上述方法构造出来的决策树还有很多的提升空间，比如说剪枝来预防过拟合问题，或者引入信息增益比的概念来解决信息增益准则对于可取值数目较多的属性有所偏好的问题。这都属于决策树的进阶技巧了，我将另开一篇文章对其进行通俗易懂的解释。对于最基本的决策树，我想说上述计算过程其实是一种极大似然估计，至于什么是极大似然估计让我们在下一篇文章《线性回归与逻辑回归》中见。</p>
<p>最后，本人水平有限，机器学习也是才入门的水平，难免有谬误，欢迎大家指正。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>CAWI2016-行为识别课程2-机器学习-决策树分类PPT</p>
<p>算法杂货铺——分类算法之决策树（Decision Tree）</p>
<p>数学之美</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/Pic.jpg"
                alt="Magina" />
            
              <p class="site-author-name" itemprop="name">Magina</p>
              <p class="site-description motion-element" itemprop="description">你自由的样子，他们说你像疯子</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Magina</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
